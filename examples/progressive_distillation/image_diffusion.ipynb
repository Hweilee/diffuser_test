{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from diffusers import AutoencoderKL, UNet2DModel, DDIMPipeline, DDIMScheduler\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.training_utils import EMAModel\n",
    "import math\n",
    "import requests\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    InterpolationMode,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "from accelerate import Accelerator\n",
    "import utils\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = utils.DiffusionTrainingArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image of my dog for this example\n",
    "\n",
    "image_url = \"https://i.imgur.com/IJcs4Aa.jpeg\"\n",
    "image = Image.open(requests.get(image_url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transforms to apply to the image for training\n",
    "augmentations = utils.get_train_transforms(training_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image = augmentations(image.convert(\"RGB\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet2DModel.from_pretrained(\"bglick13/minnie-diffusion\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s, ema_decay=0, loss=0.0551, lr=0.000299, step=1]\n",
      "Epoch 1: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0, loss=0.271, lr=0.000298, step=2]\n",
      "Epoch 2: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s, ema_decay=0.405, loss=0.321, lr=0.000297, step=3]\n",
      "Epoch 3: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s, ema_decay=0.561, loss=0.245, lr=0.000296, step=4]\n",
      "Epoch 4: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.646, loss=0.117, lr=0.000295, step=5]\n",
      "Epoch 5: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.701, loss=0.127, lr=0.000294, step=6]\n",
      "Epoch 6: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.739, loss=0.0749, lr=0.000293, step=7]\n",
      "Epoch 7: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, ema_decay=0.768, loss=0.0903, lr=0.000292, step=8]\n",
      "Epoch 8: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.79, loss=0.0618, lr=0.000291, step=9]\n",
      "Epoch 9: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s, ema_decay=0.808, loss=0.0591, lr=0.00029, step=10]\n",
      "Epoch 10: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.822, loss=0.0587, lr=0.000289, step=11]\n",
      "Epoch 11: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, ema_decay=0.834, loss=0.064, lr=0.000288, step=12]\n",
      "Epoch 12: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s, ema_decay=0.845, loss=0.0532, lr=0.000287, step=13]\n",
      "Epoch 13: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s, ema_decay=0.854, loss=0.0532, lr=0.000286, step=14]\n",
      "Epoch 14: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s, ema_decay=0.862, loss=0.0454, lr=0.000285, step=15]\n",
      "Epoch 15: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s, ema_decay=0.869, loss=0.0596, lr=0.000284, step=16]\n",
      "Epoch 16: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.875, loss=0.0493, lr=0.000283, step=17]\n",
      "Epoch 17: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, ema_decay=0.881, loss=0.0497, lr=0.000282, step=18]\n",
      "Epoch 18: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, ema_decay=0.886, loss=0.0391, lr=0.000281, step=19]\n",
      "Epoch 19: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.89, loss=0.0442, lr=0.00028, step=20]\n",
      "Epoch 20: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.894, loss=0.0519, lr=0.000279, step=21]\n",
      "Epoch 21: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.898, loss=0.0468, lr=0.000278, step=22]\n",
      "Epoch 22: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.902, loss=0.0442, lr=0.000277, step=23]\n",
      "Epoch 23: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.905, loss=0.0378, lr=0.000276, step=24]\n",
      "Epoch 24: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.908, loss=0.0383, lr=0.000275, step=25]\n",
      "Epoch 25: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, ema_decay=0.911, loss=0.0502, lr=0.000274, step=26]\n",
      "Epoch 26: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s, ema_decay=0.913, loss=0.0449, lr=0.000273, step=27]\n",
      "Epoch 27: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.916, loss=0.0377, lr=0.000272, step=28]\n",
      "Epoch 28: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s, ema_decay=0.918, loss=0.0364, lr=0.000271, step=29]\n",
      "Epoch 29: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.92, loss=0.0339, lr=0.00027, step=30]\n",
      "Epoch 30: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s, ema_decay=0.922, loss=0.0374, lr=0.000269, step=31]\n",
      "Epoch 31: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, ema_decay=0.924, loss=0.0334, lr=0.000268, step=32]\n",
      "Epoch 32: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, ema_decay=0.926, loss=0.0448, lr=0.000267, step=33]\n",
      "Epoch 33: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, ema_decay=0.927, loss=0.0365, lr=0.000266, step=34]\n",
      "Epoch 34: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, ema_decay=0.929, loss=0.0334, lr=0.000265, step=35]\n",
      "Epoch 35: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, ema_decay=0.931, loss=0.0338, lr=0.000264, step=36]\n",
      "Epoch 36: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, ema_decay=0.932, loss=0.0318, lr=0.000263, step=37]\n",
      "Epoch 37: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.933, loss=0.0383, lr=0.000262, step=38]\n",
      "Epoch 38: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.935, loss=0.0317, lr=0.000261, step=39]\n",
      "Epoch 39: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, ema_decay=0.936, loss=0.0331, lr=0.00026, step=40]\n",
      "Epoch 40: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, ema_decay=0.937, loss=0.0338, lr=0.000259, step=41]\n",
      "Epoch 41: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, ema_decay=0.938, loss=0.0355, lr=0.000258, step=42]\n",
      "Epoch 42: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.939, loss=0.0342, lr=0.000257, step=43]\n",
      "Epoch 43: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, ema_decay=0.94, loss=0.0301, lr=0.000256, step=44]\n",
      "Epoch 44: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, ema_decay=0.941, loss=0.0348, lr=0.000255, step=45]\n",
      "Epoch 45: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.942, loss=0.0327, lr=0.000254, step=46]\n",
      "Epoch 46: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.943, loss=0.0314, lr=0.000253, step=47]\n",
      "Epoch 47: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.944, loss=0.0337, lr=0.000252, step=48]\n",
      "Epoch 48: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, ema_decay=0.945, loss=0.03, lr=0.000251, step=49]\n",
      "Epoch 49: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, ema_decay=0.946, loss=0.0339, lr=0.00025, step=50]\n",
      "Epoch 50: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.947, loss=0.0344, lr=0.000249, step=51]\n",
      "Epoch 51: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.948, loss=0.0342, lr=0.000248, step=52]\n",
      "Epoch 52: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.948, loss=0.0321, lr=0.000247, step=53]\n",
      "Epoch 53: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, ema_decay=0.949, loss=0.0318, lr=0.000246, step=54]\n",
      "Epoch 54: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.95, loss=0.0338, lr=0.000245, step=55]\n",
      "Epoch 55: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, ema_decay=0.95, loss=0.0314, lr=0.000244, step=56]\n",
      "Epoch 56: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.951, loss=0.0334, lr=0.000243, step=57]\n",
      "Epoch 57: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.952, loss=0.0341, lr=0.000242, step=58]\n",
      "Epoch 58: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.952, loss=0.0278, lr=0.000241, step=59]\n",
      "Epoch 59: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.953, loss=0.0316, lr=0.00024, step=60]\n",
      "Epoch 60: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.954, loss=0.0363, lr=0.000239, step=61]\n",
      "Epoch 61: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, ema_decay=0.954, loss=0.029, lr=0.000238, step=62]\n",
      "Epoch 62: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.955, loss=0.0299, lr=0.000237, step=63]\n",
      "Epoch 63: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.955, loss=0.0319, lr=0.000236, step=64]\n",
      "Epoch 64: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.956, loss=0.0359, lr=0.000235, step=65]\n",
      "Epoch 65: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.956, loss=0.0271, lr=0.000234, step=66]\n",
      "Epoch 66: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.957, loss=0.0304, lr=0.000233, step=67]\n",
      "Epoch 67: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.957, loss=0.0332, lr=0.000232, step=68]\n",
      "Epoch 68: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.958, loss=0.0291, lr=0.000231, step=69]\n",
      "Epoch 69: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.958, loss=0.0242, lr=0.00023, step=70]\n",
      "Epoch 70: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, ema_decay=0.959, loss=0.0309, lr=0.000229, step=71]\n",
      "Epoch 71: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.959, loss=0.0291, lr=0.000228, step=72]\n",
      "Epoch 72: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, ema_decay=0.96, loss=0.027, lr=0.000227, step=73]\n",
      "Epoch 73: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.96, loss=0.0262, lr=0.000226, step=74]\n",
      "Epoch 74: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.96, loss=0.0284, lr=0.000225, step=75]\n",
      "Epoch 75: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s, ema_decay=0.961, loss=0.0281, lr=0.000224, step=76]\n",
      "Epoch 76: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.961, loss=0.0269, lr=0.000223, step=77]\n",
      "Epoch 77: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.962, loss=0.0291, lr=0.000222, step=78]\n",
      "Epoch 78: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.962, loss=0.0286, lr=0.000221, step=79]\n",
      "Epoch 79: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.962, loss=0.0264, lr=0.00022, step=80]\n",
      "Epoch 80: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.963, loss=0.0274, lr=0.000219, step=81]\n",
      "Epoch 81: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.963, loss=0.0249, lr=0.000218, step=82]\n",
      "Epoch 82: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, ema_decay=0.963, loss=0.0245, lr=0.000217, step=83]\n",
      "Epoch 83: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, ema_decay=0.964, loss=0.0297, lr=0.000216, step=84]\n",
      "Epoch 84: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, ema_decay=0.964, loss=0.028, lr=0.000215, step=85]\n",
      "Epoch 85: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.964, loss=0.0258, lr=0.000214, step=86]\n",
      "Epoch 86: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s, ema_decay=0.965, loss=0.0258, lr=0.000213, step=87]\n",
      "Epoch 87: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, ema_decay=0.965, loss=0.0268, lr=0.000212, step=88]\n",
      "Epoch 88: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.965, loss=0.029, lr=0.000211, step=89]\n",
      "Epoch 89: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.965, loss=0.0276, lr=0.00021, step=90]\n",
      "Epoch 90: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, ema_decay=0.966, loss=0.025, lr=0.000209, step=91]\n",
      "Epoch 91: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.966, loss=0.0236, lr=0.000208, step=92]\n",
      "Epoch 92: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s, ema_decay=0.966, loss=0.0269, lr=0.000207, step=93]\n",
      "Epoch 93: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.967, loss=0.0231, lr=0.000206, step=94]\n",
      "Epoch 94: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.967, loss=0.0263, lr=0.000205, step=95]\n",
      "Epoch 95: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s, ema_decay=0.967, loss=0.024, lr=0.000204, step=96]\n",
      "Epoch 96: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.967, loss=0.0252, lr=0.000203, step=97]\n",
      "Epoch 97: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, ema_decay=0.968, loss=0.0248, lr=0.000202, step=98]\n",
      "Epoch 98: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, ema_decay=0.968, loss=0.022, lr=0.000201, step=99]\n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.968, loss=0.0231, lr=0.0002, step=100]\n",
      "Epoch 100: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.968, loss=0.0241, lr=0.000199, step=101]\n",
      "Epoch 101: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.969, loss=0.0237, lr=0.000198, step=102]\n",
      "Epoch 102: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.969, loss=0.0243, lr=0.000197, step=103]\n",
      "Epoch 103: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.969, loss=0.0239, lr=0.000196, step=104]\n",
      "Epoch 104: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.969, loss=0.0268, lr=0.000195, step=105]\n",
      "Epoch 105: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.97, loss=0.0232, lr=0.000194, step=106]\n",
      "Epoch 106: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s, ema_decay=0.97, loss=0.0199, lr=0.000193, step=107]\n",
      "Epoch 107: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s, ema_decay=0.97, loss=0.0233, lr=0.000192, step=108]\n",
      "Epoch 108: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.97, loss=0.0198, lr=0.000191, step=109]\n",
      "Epoch 109: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.97, loss=0.0211, lr=0.00019, step=110]\n",
      "Epoch 110: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.971, loss=0.0219, lr=0.000189, step=111]\n",
      "Epoch 111: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.971, loss=0.0226, lr=0.000188, step=112]\n",
      "Epoch 112: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, ema_decay=0.971, loss=0.022, lr=0.000187, step=113]\n",
      "Epoch 113: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.971, loss=0.0237, lr=0.000186, step=114]\n",
      "Epoch 114: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.971, loss=0.0233, lr=0.000185, step=115]\n",
      "Epoch 115: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.972, loss=0.0204, lr=0.000184, step=116]\n",
      "Epoch 116: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.972, loss=0.0231, lr=0.000183, step=117]\n",
      "Epoch 117: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.972, loss=0.0187, lr=0.000182, step=118]\n",
      "Epoch 118: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.972, loss=0.0182, lr=0.000181, step=119]\n",
      "Epoch 119: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.972, loss=0.0215, lr=0.00018, step=120]\n",
      "Epoch 120: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.972, loss=0.0221, lr=0.000179, step=121]\n",
      "Epoch 121: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.973, loss=0.0208, lr=0.000178, step=122]\n",
      "Epoch 122: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.973, loss=0.022, lr=0.000177, step=123]\n",
      "Epoch 123: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s, ema_decay=0.973, loss=0.024, lr=0.000176, step=124]\n",
      "Epoch 124: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.973, loss=0.023, lr=0.000175, step=125]\n",
      "Epoch 125: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.973, loss=0.023, lr=0.000174, step=126]\n",
      "Epoch 126: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.973, loss=0.0207, lr=0.000173, step=127]\n",
      "Epoch 127: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.974, loss=0.0178, lr=0.000172, step=128]\n",
      "Epoch 128: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, ema_decay=0.974, loss=0.0223, lr=0.000171, step=129]\n",
      "Epoch 129: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.974, loss=0.023, lr=0.00017, step=130]\n",
      "Epoch 130: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.974, loss=0.0246, lr=0.000169, step=131]\n",
      "Epoch 131: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.974, loss=0.0199, lr=0.000168, step=132]\n",
      "Epoch 132: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.974, loss=0.0201, lr=0.000167, step=133]\n",
      "Epoch 133: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.974, loss=0.0202, lr=0.000166, step=134]\n",
      "Epoch 134: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.975, loss=0.0186, lr=0.000165, step=135]\n",
      "Epoch 135: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, ema_decay=0.975, loss=0.0216, lr=0.000164, step=136]\n",
      "Epoch 136: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, ema_decay=0.975, loss=0.0227, lr=0.000163, step=137]\n",
      "Epoch 137: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.975, loss=0.0168, lr=0.000162, step=138]\n",
      "Epoch 138: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s, ema_decay=0.975, loss=0.0239, lr=0.000161, step=139]\n",
      "Epoch 139: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s, ema_decay=0.975, loss=0.0208, lr=0.00016, step=140]\n",
      "Epoch 140: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s, ema_decay=0.975, loss=0.0217, lr=0.000159, step=141]\n",
      "Epoch 141: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.976, loss=0.0206, lr=0.000158, step=142]\n",
      "Epoch 142: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s, ema_decay=0.976, loss=0.0206, lr=0.000157, step=143]\n",
      "Epoch 143: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.976, loss=0.017, lr=0.000156, step=144]\n",
      "Epoch 144: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.976, loss=0.0169, lr=0.000155, step=145]\n",
      "Epoch 145: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.976, loss=0.0193, lr=0.000154, step=146]\n",
      "Epoch 146: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, ema_decay=0.976, loss=0.0199, lr=0.000153, step=147]\n",
      "Epoch 147: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.976, loss=0.0222, lr=0.000152, step=148]\n",
      "Epoch 148: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.976, loss=0.0198, lr=0.000151, step=149]\n",
      "Epoch 149: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.977, loss=0.0201, lr=0.00015, step=150]\n",
      "Epoch 150: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s, ema_decay=0.977, loss=0.0228, lr=0.000149, step=151]\n",
      "Epoch 151: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.977, loss=0.0188, lr=0.000148, step=152]\n",
      "Epoch 152: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.977, loss=0.0194, lr=0.000147, step=153]\n",
      "Epoch 153: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s, ema_decay=0.977, loss=0.0198, lr=0.000146, step=154]\n",
      "Epoch 154: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, ema_decay=0.977, loss=0.0186, lr=0.000145, step=155]\n",
      "Epoch 155: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.977, loss=0.0161, lr=0.000144, step=156]\n",
      "Epoch 156: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.977, loss=0.0209, lr=0.000143, step=157]\n",
      "Epoch 157: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.977, loss=0.0188, lr=0.000142, step=158]\n",
      "Epoch 158: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.978, loss=0.0189, lr=0.000141, step=159]\n",
      "Epoch 159: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.978, loss=0.0208, lr=0.00014, step=160]\n",
      "Epoch 160: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.978, loss=0.0227, lr=0.000139, step=161]\n",
      "Epoch 161: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.978, loss=0.0182, lr=0.000138, step=162]\n",
      "Epoch 162: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.978, loss=0.0179, lr=0.000137, step=163]\n",
      "Epoch 163: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.978, loss=0.02, lr=0.000136, step=164]\n",
      "Epoch 164: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.978, loss=0.0201, lr=0.000135, step=165]\n",
      "Epoch 165: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.978, loss=0.0205, lr=0.000134, step=166]\n",
      "Epoch 166: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.978, loss=0.02, lr=0.000133, step=167]\n",
      "Epoch 167: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, ema_decay=0.978, loss=0.0161, lr=0.000132, step=168]\n",
      "Epoch 168: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.979, loss=0.0211, lr=0.000131, step=169]\n",
      "Epoch 169: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.979, loss=0.0195, lr=0.00013, step=170]\n",
      "Epoch 170: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s, ema_decay=0.979, loss=0.0179, lr=0.000129, step=171]\n",
      "Epoch 171: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s, ema_decay=0.979, loss=0.016, lr=0.000128, step=172]\n",
      "Epoch 172: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.979, loss=0.0198, lr=0.000127, step=173]\n",
      "Epoch 173: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.979, loss=0.0176, lr=0.000126, step=174]\n",
      "Epoch 174: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.979, loss=0.0174, lr=0.000125, step=175]\n",
      "Epoch 175: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.979, loss=0.0219, lr=0.000124, step=176]\n",
      "Epoch 176: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.979, loss=0.0205, lr=0.000123, step=177]\n",
      "Epoch 177: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, ema_decay=0.979, loss=0.016, lr=0.000122, step=178]\n",
      "Epoch 178: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.979, loss=0.0195, lr=0.000121, step=179]\n",
      "Epoch 179: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, ema_decay=0.98, loss=0.019, lr=0.00012, step=180]\n",
      "Epoch 180: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.98, loss=0.0195, lr=0.000119, step=181]\n",
      "Epoch 181: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.98, loss=0.0187, lr=0.000118, step=182]\n",
      "Epoch 182: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s, ema_decay=0.98, loss=0.0188, lr=0.000117, step=183]\n",
      "Epoch 183: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.98, loss=0.0196, lr=0.000116, step=184]\n",
      "Epoch 184: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.98, loss=0.0236, lr=0.000115, step=185]\n",
      "Epoch 185: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.98, loss=0.0189, lr=0.000114, step=186]\n",
      "Epoch 186: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.98, loss=0.0203, lr=0.000113, step=187]\n",
      "Epoch 187: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.98, loss=0.022, lr=0.000112, step=188]\n",
      "Epoch 188: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.98, loss=0.0181, lr=0.000111, step=189]\n",
      "Epoch 189: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.98, loss=0.0168, lr=0.00011, step=190]\n",
      "Epoch 190: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, ema_decay=0.98, loss=0.023, lr=0.000109, step=191]\n",
      "Epoch 191: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.981, loss=0.0174, lr=0.000108, step=192]\n",
      "Epoch 192: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.981, loss=0.0208, lr=0.000107, step=193]\n",
      "Epoch 193: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.981, loss=0.0164, lr=0.000106, step=194]\n",
      "Epoch 194: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.981, loss=0.016, lr=0.000105, step=195]\n",
      "Epoch 195: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.981, loss=0.0183, lr=0.000104, step=196]\n",
      "Epoch 196: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.981, loss=0.0186, lr=0.000103, step=197]\n",
      "Epoch 197: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.981, loss=0.0151, lr=0.000102, step=198]\n",
      "Epoch 198: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s, ema_decay=0.981, loss=0.0173, lr=0.000101, step=199]\n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.981, loss=0.0159, lr=0.0001, step=200]\n",
      "Epoch 200: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, ema_decay=0.981, loss=0.0191, lr=9.9e-5, step=201]\n",
      "Epoch 201: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.981, loss=0.0195, lr=9.8e-5, step=202]\n",
      "Epoch 202: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s, ema_decay=0.981, loss=0.0147, lr=9.7e-5, step=203]\n",
      "Epoch 203: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s, ema_decay=0.981, loss=0.0183, lr=9.6e-5, step=204]\n",
      "Epoch 204: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s, ema_decay=0.981, loss=0.0194, lr=9.5e-5, step=205]\n",
      "Epoch 205: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.982, loss=0.0178, lr=9.4e-5, step=206]\n",
      "Epoch 206: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, ema_decay=0.982, loss=0.015, lr=9.3e-5, step=207]\n",
      "Epoch 207: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s, ema_decay=0.982, loss=0.0175, lr=9.2e-5, step=208]\n",
      "Epoch 208: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, ema_decay=0.982, loss=0.0183, lr=9.1e-5, step=209]\n",
      "Epoch 209: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.982, loss=0.0172, lr=9e-5, step=210]\n",
      "Epoch 210: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s, ema_decay=0.982, loss=0.0183, lr=8.9e-5, step=211]\n",
      "Epoch 211: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s, ema_decay=0.982, loss=0.019, lr=8.8e-5, step=212]\n",
      "Epoch 212: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.982, loss=0.0163, lr=8.7e-5, step=213]\n",
      "Epoch 213: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s, ema_decay=0.982, loss=0.0168, lr=8.6e-5, step=214]\n",
      "Epoch 214: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.982, loss=0.0181, lr=8.5e-5, step=215]\n",
      "Epoch 215: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, ema_decay=0.982, loss=0.0175, lr=8.4e-5, step=216]\n",
      "Epoch 216: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.982, loss=0.0183, lr=8.3e-5, step=217]\n",
      "Epoch 217: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, ema_decay=0.982, loss=0.0184, lr=8.2e-5, step=218]\n",
      "Epoch 218: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.982, loss=0.0158, lr=8.1e-5, step=219]\n",
      "Epoch 219: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.982, loss=0.0188, lr=8e-5, step=220]\n",
      "Epoch 220: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, ema_decay=0.982, loss=0.0171, lr=7.9e-5, step=221]\n",
      "Epoch 221: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.983, loss=0.0189, lr=7.8e-5, step=222]\n",
      "Epoch 222: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.983, loss=0.0168, lr=7.7e-5, step=223]\n",
      "Epoch 223: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s, ema_decay=0.983, loss=0.0159, lr=7.6e-5, step=224]\n",
      "Epoch 224: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, ema_decay=0.983, loss=0.0176, lr=7.5e-5, step=225]\n",
      "Epoch 225: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s, ema_decay=0.983, loss=0.0171, lr=7.4e-5, step=226]\n",
      "Epoch 226: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, ema_decay=0.983, loss=0.0163, lr=7.3e-5, step=227]\n",
      "Epoch 227: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s, ema_decay=0.983, loss=0.0192, lr=7.2e-5, step=228]\n",
      "Epoch 228: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, ema_decay=0.983, loss=0.016, lr=7.1e-5, step=229]\n",
      "Epoch 229: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s, ema_decay=0.983, loss=0.0173, lr=7e-5, step=230]\n",
      "Epoch 230: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.983, loss=0.0172, lr=6.9e-5, step=231]\n",
      "Epoch 231: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.983, loss=0.0148, lr=6.8e-5, step=232]\n",
      "Epoch 232: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.983, loss=0.018, lr=6.7e-5, step=233]\n",
      "Epoch 233: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.983, loss=0.0199, lr=6.6e-5, step=234]\n",
      "Epoch 234: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.983, loss=0.0179, lr=6.5e-5, step=235]\n",
      "Epoch 235: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.983, loss=0.0173, lr=6.4e-5, step=236]\n",
      "Epoch 236: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.983, loss=0.0194, lr=6.3e-5, step=237]\n",
      "Epoch 237: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.983, loss=0.0151, lr=6.2e-5, step=238]\n",
      "Epoch 238: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.983, loss=0.0221, lr=6.1e-5, step=239]\n",
      "Epoch 239: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.984, loss=0.0162, lr=6e-5, step=240]\n",
      "Epoch 240: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.984, loss=0.0184, lr=5.9e-5, step=241]\n",
      "Epoch 241: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s, ema_decay=0.984, loss=0.0174, lr=5.8e-5, step=242]\n",
      "Epoch 242: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s, ema_decay=0.984, loss=0.0187, lr=5.7e-5, step=243]\n",
      "Epoch 243: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s, ema_decay=0.984, loss=0.018, lr=5.6e-5, step=244]\n",
      "Epoch 244: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, ema_decay=0.984, loss=0.0184, lr=5.5e-5, step=245]\n",
      "Epoch 245: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, ema_decay=0.984, loss=0.0202, lr=5.4e-5, step=246]\n",
      "Epoch 246: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, ema_decay=0.984, loss=0.0184, lr=5.3e-5, step=247]\n",
      "Epoch 247: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.984, loss=0.016, lr=5.2e-5, step=248]\n",
      "Epoch 248: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.984, loss=0.0194, lr=5.1e-5, step=249]\n",
      "Epoch 249: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.984, loss=0.0193, lr=5e-5, step=250]\n",
      "Epoch 250: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s, ema_decay=0.984, loss=0.017, lr=4.9e-5, step=251]\n",
      "Epoch 251: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s, ema_decay=0.984, loss=0.0193, lr=4.8e-5, step=252]\n",
      "Epoch 252: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, ema_decay=0.984, loss=0.0187, lr=4.7e-5, step=253]\n",
      "Epoch 253: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s, ema_decay=0.984, loss=0.0149, lr=4.6e-5, step=254]\n",
      "Epoch 254: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, ema_decay=0.984, loss=0.0174, lr=4.5e-5, step=255]\n",
      "Epoch 255: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s, ema_decay=0.984, loss=0.0169, lr=4.4e-5, step=256]\n",
      "Epoch 256: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s, ema_decay=0.984, loss=0.0154, lr=4.3e-5, step=257]\n",
      "Epoch 257: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, ema_decay=0.984, loss=0.016, lr=4.2e-5, step=258]\n",
      "Epoch 258: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.984, loss=0.0217, lr=4.1e-5, step=259]\n",
      "Epoch 259: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s, ema_decay=0.985, loss=0.0188, lr=4e-5, step=260]\n",
      "Epoch 260: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, ema_decay=0.985, loss=0.0187, lr=3.9e-5, step=261]\n",
      "Epoch 261: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.985, loss=0.0144, lr=3.8e-5, step=262]\n",
      "Epoch 262: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s, ema_decay=0.985, loss=0.0161, lr=3.7e-5, step=263]\n",
      "Epoch 263: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s, ema_decay=0.985, loss=0.0123, lr=3.6e-5, step=264]\n",
      "Epoch 264: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, ema_decay=0.985, loss=0.0182, lr=3.5e-5, step=265]\n",
      "Epoch 265: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.985, loss=0.0204, lr=3.4e-5, step=266]\n",
      "Epoch 266: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s, ema_decay=0.985, loss=0.0202, lr=3.3e-5, step=267]\n",
      "Epoch 267: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s, ema_decay=0.985, loss=0.0159, lr=3.2e-5, step=268]\n",
      "Epoch 268: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.985, loss=0.0186, lr=3.1e-5, step=269]\n",
      "Epoch 269: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s, ema_decay=0.985, loss=0.0171, lr=3e-5, step=270]\n",
      "Epoch 270: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.985, loss=0.0141, lr=2.9e-5, step=271]\n",
      "Epoch 271: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, ema_decay=0.985, loss=0.0149, lr=2.8e-5, step=272]\n",
      "Epoch 272: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s, ema_decay=0.985, loss=0.0169, lr=2.7e-5, step=273]\n",
      "Epoch 273: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s, ema_decay=0.985, loss=0.0157, lr=2.6e-5, step=274]\n",
      "Epoch 274: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.985, loss=0.0177, lr=2.5e-5, step=275]\n",
      "Epoch 275: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.985, loss=0.0193, lr=2.4e-5, step=276]\n",
      "Epoch 276: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.985, loss=0.0151, lr=2.3e-5, step=277]\n",
      "Epoch 277: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.985, loss=0.0161, lr=2.2e-5, step=278]\n",
      "Epoch 278: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, ema_decay=0.985, loss=0.0153, lr=2.1e-5, step=279]\n",
      "Epoch 279: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, ema_decay=0.985, loss=0.0177, lr=2e-5, step=280]\n",
      "Epoch 280: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.985, loss=0.0173, lr=1.9e-5, step=281]\n",
      "Epoch 281: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s, ema_decay=0.985, loss=0.0172, lr=1.8e-5, step=282]\n",
      "Epoch 282: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, ema_decay=0.985, loss=0.0161, lr=1.7e-5, step=283]\n",
      "Epoch 283: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, ema_decay=0.986, loss=0.0155, lr=1.6e-5, step=284]\n",
      "Epoch 284: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.986, loss=0.0152, lr=1.5e-5, step=285]\n",
      "Epoch 285: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.986, loss=0.0159, lr=1.4e-5, step=286]\n",
      "Epoch 286: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s, ema_decay=0.986, loss=0.0132, lr=1.3e-5, step=287]\n",
      "Epoch 287: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.986, loss=0.0194, lr=1.2e-5, step=288]\n",
      "Epoch 288: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s, ema_decay=0.986, loss=0.018, lr=1.1e-5, step=289]\n",
      "Epoch 289: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, ema_decay=0.986, loss=0.0186, lr=1e-5, step=290]\n",
      "Epoch 290: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.986, loss=0.0151, lr=9e-6, step=291]\n",
      "Epoch 291: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.986, loss=0.0169, lr=8e-6, step=292]\n",
      "Epoch 292: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.986, loss=0.0163, lr=7e-6, step=293]\n",
      "Epoch 293: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, ema_decay=0.986, loss=0.0181, lr=6e-6, step=294]\n",
      "Epoch 294: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.986, loss=0.0145, lr=5e-6, step=295]\n",
      "Epoch 295: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, ema_decay=0.986, loss=0.0184, lr=4e-6, step=296]\n",
      "Epoch 296: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, ema_decay=0.986, loss=0.0168, lr=3e-6, step=297]\n",
      "Epoch 297: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s, ema_decay=0.986, loss=0.0177, lr=2e-6, step=298]\n",
      "Epoch 298: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s, ema_decay=0.986, loss=0.018, lr=1e-6, step=299]\n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s, ema_decay=0.986, loss=0.0179, lr=0, step=300]\n"
     ]
    }
   ],
   "source": [
    "def distill(teacher, n, train_image, epochs=100, lr=3e-4, batch_size=16):\n",
    "    accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=training_config.gradient_accumulation_steps,\n",
    "    mixed_precision=training_config.mixed_precision,\n",
    ")\n",
    "    if accelerator.is_main_process:\n",
    "        run = \"distill\"\n",
    "        accelerator.init_trackers(run)\n",
    "    teacher_scheduler = DDIMScheduler(num_train_timesteps=n)\n",
    "    student_scheduler = DDIMScheduler(num_train_timesteps=n // 2)\n",
    "    student = utils.get_unet(training_config)\n",
    "    student.load_state_dict(teacher.state_dict())\n",
    "    student = accelerator.prepare(student)\n",
    "    student.train()\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        student.parameters(),\n",
    "        lr=lr,\n",
    "        betas=(training_config.adam_beta1, training_config.adam_beta2),\n",
    "        weight_decay=0.001,\n",
    "        eps=training_config.adam_epsilon,\n",
    "    )\n",
    "    lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(epochs) // training_config.gradient_accumulation_steps,\n",
    ")\n",
    "    teacher, student, optimizer, lr_scheduler, train_image, teacher_scheduler, student_scheduler = accelerator.prepare(\n",
    "    teacher, student, optimizer, lr_scheduler, train_image,teacher_scheduler, student_scheduler\n",
    ")\n",
    "    ema_model = EMAModel(student, inv_gamma=training_config.ema_inv_gamma, power=training_config.ema_power, max_value=training_config.ema_max_decay)\n",
    "    global_step = 0\n",
    "    for epoch in range(epochs):\n",
    "        progress_bar = tqdm(total=1, disable=not accelerator.is_local_main_process)\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "        batch = train_image.unsqueeze(0).repeat(\n",
    "            batch_size, 1, 1, 1\n",
    "        ).to(accelerator.device)\n",
    "        with accelerator.accumulate(student):\n",
    "            noise = torch.randn(batch.shape).to(accelerator.device)\n",
    "            bsz = batch.shape[0]\n",
    "            # Sample a random timestep for each image\n",
    "            timesteps = torch.randint(\n",
    "                2, student_scheduler.config.num_train_timesteps, (bsz,), device=batch.device\n",
    "            ).long() * 2\n",
    "            with torch.no_grad():\n",
    "                alpha_t, sigma_t = teacher_scheduler.get_alpha_sigma(batch, timesteps, accelerator.device)\n",
    "                z_t = alpha_t * batch + sigma_t * noise\n",
    "                alpha_t_prime2, sigma_t_prime2 = teacher_scheduler.get_alpha_sigma(batch, timesteps-2, accelerator.device)\n",
    "                alpha_t_prime, sigma_t_prime = teacher_scheduler.get_alpha_sigma(batch, timesteps-1, accelerator.device)\n",
    "                noise_pred_t = teacher(z_t, timesteps).sample\n",
    "                x_teacher_z_t = (alpha_t * z_t - sigma_t * noise_pred_t).clip(-1, 1)\n",
    "\n",
    "                z_t_prime = alpha_t_prime * x_teacher_z_t + (sigma_t_prime / sigma_t) * (z_t - alpha_t * x_teacher_z_t)\n",
    "                noise_pred_t_prime = teacher(z_t_prime.float(), timesteps - 1).sample\n",
    "                rec_t_prime = (alpha_t_prime * z_t_prime - sigma_t_prime * noise_pred_t_prime).clip(-1, 1)\n",
    "\n",
    "                x_teacher_z_t_prime = (z_t - alpha_t_prime2 * rec_t_prime) / sigma_t_prime2\n",
    "                z_t_prime_2 = alpha_t_prime2 * x_teacher_z_t_prime - sigma_t_prime2 * rec_t_prime\n",
    "\n",
    "            noise_pred = student(z_t, timesteps).sample\n",
    "            loss = F.mse_loss(noise_pred, z_t_prime_2)\n",
    "            accelerator.backward(loss)\n",
    "\n",
    "            if accelerator.sync_gradients:\n",
    "                accelerator.clip_grad_norm_(student.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            if training_config.use_ema:\n",
    "                ema_model.step(student)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "        if accelerator.sync_gradients:\n",
    "            progress_bar.update(1)\n",
    "            global_step += 1\n",
    "\n",
    "        logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
    "        if training_config.use_ema:\n",
    "            logs[\"ema_decay\"] = ema_model.decay\n",
    "        progress_bar.set_postfix(**logs)\n",
    "        accelerator.log(logs, step=global_step)\n",
    "        progress_bar.close()\n",
    "\n",
    "        accelerator.wait_for_everyone()\n",
    "    return student, ema_model, accelerator\n",
    "teacher, distilled_ema, distill_accelrator = distill(model, 1000, train_image, epochs=300, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1fd615f0a184cf8811bbe59f728e4c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_scheduler = DDIMScheduler(num_train_timesteps=500)\n",
    "pipeline = DDIMPipeline(\n",
    "    unet=distill_accelrator.unwrap_model(distilled_ema.averaged_model if training_config.use_ema else teacher),\n",
    "    scheduler=new_scheduler,\n",
    ")\n",
    "\n",
    "generator = torch.manual_seed(0)\n",
    "# run pipeline in inference (sample random noise and denoise)\n",
    "images = pipeline(generator=generator, batch_size=training_config.batch_size, output_type=\"numpy\").images\n",
    "\n",
    "# denormalize the images and save to tensorboard\n",
    "images_processed = (images * 255).round().astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image.fromarray(images_processed[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
