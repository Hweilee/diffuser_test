{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from diffusers import AutoencoderKL, UNet2DModel, DDPMScheduler, DDPMPipeline\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.training_utils import EMAModel\n",
    "import math\n",
    "import requests\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    InterpolationMode,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "from accelerate import Accelerator\n",
    "import utils\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = utils.DiffusionTrainingArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image of my dog for this example\n",
    "\n",
    "image_url = \"https://i.imgur.com/IJcs4Aa.jpeg\"\n",
    "image = Image.open(requests.get(image_url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transforms to apply to the image for training\n",
    "augmentations = utils.get_train_transforms(training_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image = augmentations(image.convert(\"RGB\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet2DModel.from_pretrained(\"bglick13/minnie-diffusion\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A/tmp/ipykernel_602221/3684360613.py:49: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  alpha_t_prime2, sigma_t_prime2 = student_scheduler.get_alpha_sigma(batch, timesteps // 2, accelerator.device)\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Epoch 0: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s, ema_decay=0, loss=0.461, lr=0.000297, step=1]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Epoch 1: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s, ema_decay=0, loss=0.418, lr=0.000294, step=2]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Epoch 2: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s, ema_decay=0.405, loss=0.308, lr=0.000291, step=3]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Epoch 3: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s, ema_decay=0.561, loss=0.368, lr=0.000288, step=4]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Epoch 4: 100%|██████████| 1/1 [00:00<00:00,  1.32it/s, ema_decay=0.646, loss=0.278, lr=0.000285, step=5]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Epoch 5: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s, ema_decay=0.701, loss=0.255, lr=0.000282, step=6]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Epoch 6: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s, ema_decay=0.739, loss=0.199, lr=0.000279, step=7]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Epoch 7: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s, ema_decay=0.768, loss=0.227, lr=0.000276, step=8]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Epoch 8: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s, ema_decay=0.79, loss=0.298, lr=0.000273, step=9]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Epoch 9: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s, ema_decay=0.808, loss=0.212, lr=0.00027, step=10]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Epoch 10: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s, ema_decay=0.822, loss=0.254, lr=0.000267, step=11]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Epoch 11: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s, ema_decay=0.834, loss=0.23, lr=0.000264, step=12]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Epoch 12: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s, ema_decay=0.845, loss=0.129, lr=0.000261, step=13]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Epoch 13: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s, ema_decay=0.854, loss=0.158, lr=0.000258, step=14]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Epoch 14: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s, ema_decay=0.862, loss=0.137, lr=0.000255, step=15]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Epoch 15: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s, ema_decay=0.869, loss=0.171, lr=0.000252, step=16]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_602221/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">3684360613.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">90</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_602221/3684360613.py'</span>                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_602221/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">3684360613.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">66</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">distill</span>                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_602221/3684360613.py'</span>                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/ben/.local/lib/python3.8/site-packages/accelerate/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">accelerator.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1005</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1002 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.distributed_type == DistributedType.DEEPSPEED:                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1003 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.deepspeed_engine_wrapped.backward(loss, **kwargs)                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1004 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.scaler <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1005 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.scaler.scale(loss).backward(**kwargs)                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1006 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1007 │   │   │   </span>loss.backward(**kwargs)                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1008 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/ben/.local/lib/python3.8/site-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_tensor.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">396</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 393 │   │   │   │   </span>retain_graph=retain_graph,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 394 │   │   │   │   </span>create_graph=create_graph,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 395 │   │   │   │   </span>inputs=inputs)                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 396 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>torch.autograd.backward(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, gradient, retain_graph, create_graph, inputs=input  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 397 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 398 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">register_hook</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, hook):                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 399 │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">r\"\"\"Registers a backward hook.</span>                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/ben/.local/lib/python3.8/site-packages/torch/autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">173</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">170 │   # The reason we repeat same the comment below is that</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">171 │   # some Python versions print out the first line of a multi-line function</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">172 │   # calls in the traceback and some print out the last line</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>173 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>Variable._execution_engine.run_backward(  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to run the bac</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">174 │   │   </span>tensors, grad_tensors_, retain_graph, create_graph, inputs,                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">175 │   │   </span>allow_unreachable=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, accumulate_grad=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to ru</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_602221/\u001b[0m\u001b[1;33m3684360613.py\u001b[0m:\u001b[94m90\u001b[0m in \u001b[92m<module>\u001b[0m                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_602221/3684360613.py'\u001b[0m                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_602221/\u001b[0m\u001b[1;33m3684360613.py\u001b[0m:\u001b[94m66\u001b[0m in \u001b[92mdistill\u001b[0m                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_602221/3684360613.py'\u001b[0m                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/ben/.local/lib/python3.8/site-packages/accelerate/\u001b[0m\u001b[1;33maccelerator.py\u001b[0m:\u001b[94m1005\u001b[0m in \u001b[92mbackward\u001b[0m          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1002 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.distributed_type == DistributedType.DEEPSPEED:                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1003 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.deepspeed_engine_wrapped.backward(loss, **kwargs)                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1004 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melif\u001b[0m \u001b[96mself\u001b[0m.scaler \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1005 \u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.scaler.scale(loss).backward(**kwargs)                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1006 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1007 \u001b[0m\u001b[2m│   │   │   \u001b[0mloss.backward(**kwargs)                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1008 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/ben/.local/lib/python3.8/site-packages/torch/\u001b[0m\u001b[1;33m_tensor.py\u001b[0m:\u001b[94m396\u001b[0m in \u001b[92mbackward\u001b[0m                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 393 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mretain_graph=retain_graph,                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 394 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcreate_graph=create_graph,                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 395 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minputs=inputs)                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 396 \u001b[2m│   │   \u001b[0mtorch.autograd.backward(\u001b[96mself\u001b[0m, gradient, retain_graph, create_graph, inputs=input  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 397 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 398 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mregister_hook\u001b[0m(\u001b[96mself\u001b[0m, hook):                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 399 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[33mr\u001b[0m\u001b[33m\"\"\"Registers a backward hook.\u001b[0m                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/ben/.local/lib/python3.8/site-packages/torch/autograd/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m173\u001b[0m in \u001b[92mbackward\u001b[0m          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m170 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# The reason we repeat same the comment below is that\u001b[0m                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m171 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# some Python versions print out the first line of a multi-line function\u001b[0m               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m172 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# calls in the traceback and some print out the last line\u001b[0m                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m173 \u001b[2m│   \u001b[0mVariable._execution_engine.run_backward(  \u001b[2m# Calls into the C++ engine to run the bac\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m174 \u001b[0m\u001b[2m│   │   \u001b[0mtensors, grad_tensors_, retain_graph, create_graph, inputs,                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m175 \u001b[0m\u001b[2m│   │   \u001b[0mallow_unreachable=\u001b[94mTrue\u001b[0m, accumulate_grad=\u001b[94mTrue\u001b[0m)  \u001b[2m# Calls into the C++ engine to ru\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m176 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def distill(teacher, n, train_image, epochs=100, lr=3e-4, batch_size=16):\n",
    "    accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=training_config.gradient_accumulation_steps,\n",
    "    mixed_precision=training_config.mixed_precision,\n",
    ")\n",
    "    if accelerator.is_main_process:\n",
    "        run = \"distill\"\n",
    "        accelerator.init_trackers(run)\n",
    "    teacher_scheduler = DDPMScheduler(num_train_timesteps=n)\n",
    "    student_scheduler = DDPMScheduler(num_train_timesteps=n // 2)\n",
    "    student = utils.get_unet(training_config)\n",
    "    student.load_state_dict(teacher.state_dict())\n",
    "    student = accelerator.prepare(student)\n",
    "    student.train()\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        student.parameters(),\n",
    "        lr=lr,\n",
    "        betas=(training_config.adam_beta1, training_config.adam_beta2),\n",
    "        weight_decay=0.001,\n",
    "        eps=training_config.adam_epsilon,\n",
    "    )\n",
    "    lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(epochs) // training_config.gradient_accumulation_steps,\n",
    ")\n",
    "    teacher, student, optimizer, lr_scheduler, train_image, teacher_scheduler, student_scheduler = accelerator.prepare(\n",
    "    teacher, student, optimizer, lr_scheduler, train_image,teacher_scheduler, student_scheduler\n",
    ")\n",
    "    ema_model = EMAModel(student, inv_gamma=training_config.ema_inv_gamma, power=training_config.ema_power, max_value=training_config.ema_max_decay)\n",
    "    global_step = 0\n",
    "    for epoch in range(epochs):\n",
    "        progress_bar = tqdm(total=1, disable=not accelerator.is_local_main_process)\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "        batch = train_image.unsqueeze(0).repeat(\n",
    "            batch_size, 1, 1, 1\n",
    "        ).to(accelerator.device)\n",
    "        with accelerator.accumulate(student):\n",
    "            noise = torch.randn(batch.shape).to(accelerator.device)\n",
    "            bsz = batch.shape[0]\n",
    "            # Sample a random timestep for each image\n",
    "            timesteps = torch.randint(\n",
    "                0, student_scheduler.config.num_train_timesteps, (bsz,), device=batch.device\n",
    "            ).long() * 2\n",
    "            with torch.no_grad():\n",
    "                alpha_t, sigma_t = teacher_scheduler.get_alpha_sigma(batch, timesteps + 1, accelerator.device)\n",
    "                z_t = alpha_t * batch + sigma_t * noise\n",
    "                alpha_t_prime2, sigma_t_prime2 = student_scheduler.get_alpha_sigma(batch, timesteps // 2, accelerator.device)\n",
    "                alpha_t_prime, sigma_t_prime = teacher_scheduler.get_alpha_sigma(batch, timesteps, accelerator.device)\n",
    "                v = teacher(z_t.float(), timesteps + 1).sample\n",
    "                rec_t = (alpha_t * z_t - sigma_t * v).clip(-1, 1)\n",
    "\n",
    "                z_t_prime = alpha_t_prime * rec_t + (sigma_t_prime / sigma_t) * (z_t - alpha_t * rec_t)\n",
    "                v_1 = teacher(z_t_prime.float(), timesteps).sample\n",
    "                rec_t_prime = (alpha_t_prime * z_t_prime - sigma_t_prime * v_1).clip(-1, 1)\n",
    "                z_t_prime_2 = alpha_t_prime2 * rec_t_prime + (sigma_t_prime2 / sigma_t_prime) * (z_t_prime - alpha_t_prime * rec_t_prime)\n",
    "                x_hat = z_t_prime_2 - ((sigma_t_prime2 / sigma_t_prime) * z_t) / (alpha_t_prime2 - (sigma_t_prime2 / sigma_t_prime) * alpha_t)\n",
    "\n",
    "            noise_pred = student(z_t, timesteps).sample\n",
    "            student_rec = (alpha_t * z_t - sigma_t * noise_pred).clip(-1, 1)\n",
    "            loss = F.mse_loss(student_rec, x_hat.clip(-1, 1))\n",
    "            \n",
    "            accelerator.backward(loss)\n",
    "\n",
    "            if accelerator.sync_gradients:\n",
    "                accelerator.clip_grad_norm_(student.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            if training_config.use_ema:\n",
    "                ema_model.step(student)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "        if accelerator.sync_gradients:\n",
    "            progress_bar.update(1)\n",
    "            global_step += 1\n",
    "\n",
    "        logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
    "        if training_config.use_ema:\n",
    "            logs[\"ema_decay\"] = ema_model.decay\n",
    "        progress_bar.set_postfix(**logs)\n",
    "        accelerator.log(logs, step=global_step)\n",
    "        progress_bar.close()\n",
    "\n",
    "        accelerator.wait_for_everyone()\n",
    "    return student, ema_model, accelerator\n",
    "teacher, distilled_ema, distill_accelrator = distill(model, 1000, train_image, epochs=100, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8921e5a95394b7e888ba3af9fe4f0b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_scheduler = DDPMScheduler(num_train_timesteps=500)\n",
    "pipeline = DDPMPipeline(\n",
    "    unet=distill_accelrator.unwrap_model(distilled_ema.averaged_model if training_config.use_ema else teacher),\n",
    "    scheduler=new_scheduler,\n",
    ")\n",
    "\n",
    "generator = torch.manual_seed(0)\n",
    "# run pipeline in inference (sample random noise and denoise)\n",
    "images = pipeline(generator=generator, batch_size=training_config.batch_size, output_type=\"numpy\").images\n",
    "\n",
    "# denormalize the images and save to tensorboard\n",
    "images_processed = (images * 255).round().astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image.fromarray(images_processed[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
