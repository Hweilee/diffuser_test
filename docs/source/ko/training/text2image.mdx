<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->


# Stable Diffusion text-to-image 미세 조정

[`train_text_to_image.py`](https://github.com/huggingface/diffusers/tree/main/examples/text_to_image) 스크립트는 자체 데이터셋에서 stable diffusion 모델을 미세 조정하는 방법을 보여줍니다.

<Tip warning={true}>

text-to-image 미세 조정 스크립트는 실험적입니다. 과적합하기 쉽고 치명적인 망각과 같은 문제에 부딪히기 쉽습니다. 자체 데이터셋에서 최상의 결과를 얻으려면 다양한 하이퍼파라미터를 탐색하는 것이 좋습니다.
</Tip>


## 로컬에서 실행하기

### dependency 설치하기

스크립트를 실행하기 전에, 라이브러리의 학습 dependency들을 설치해야 합니다:

```bash
pip install git+https://github.com/huggingface/diffusers.git
pip install -U -r requirements.txt
```

그리고 [🤗가속](https://github.com/huggingface/accelerate/) 환경을 초기화합니다:

```bash
accelerate config
```

가중치를 다운로드하거나 사용하기 전에 모델 라이선스에 동의해야 합니다. 이 예시에서는 모델 버전 `v1-4`를 사용하므로, [이 링크](https://huggingface.co/CompVis/stable-diffusion-v1-4)를 방문하여 라이선스를 읽고 동의하면 확인란을 선택하십시오.

🤗 Hugging Face Hub에 등록된 사용자여야 하며, 코드를 작동하기 위해 액세스 토큰도 사용해야 합니다. 액세스 토큰에 대한 자세한 내용은 [문서의 이 섹션](https://huggingface.co/docs/hub/security-tokens)을 참조하세요.

다음 명령을 실행하여 토큰을 인증합니다.

```bash
huggingface-cli login
```

리포지토리를 이미 복제한 경우, 이 단계를 수행할 필요가 없습니다. 대신, 로컬 체크아웃 경로를 학습 스크립트에 명시할 수 있으며 거기에서 로드됩니다.

### 미세 조정을 위한 하드웨어 요구 사항

`gradient_checkpointing` 및 `mixed_precision`을 사용하면 단일 24GB GPU에서 모델을 미세 조정할 수 있습니다. 더 높은 `batch_size`와 더 빠른 훈련을 위해서는 GPU 메모리가 30GB 이상인 GPU를 사용하는 것이 좋습니다. TPU 또는 GPU에서 미세 조정을 위해 JAX나 Flax를 사용할 수도 있습니다. 자세한 내용은 [아래](#flax-jax-finetuning)를 참조하세요.

### 미세 조정 예시

다음 스크립트는 Hugging Face Hub에서 사용할 수 있는 [Justin Pinkneys이 캡션한 Pokemon 데이터셋](https://huggingface.co/datasets/lambdalabs/pokemon-blip-captions)를 사용하여 미세 조정 실행을 시작합니다.

```bash
export MODEL_NAME="CompVis/stable-diffusion-v1-4"
export dataset_name="lambdalabs/pokemon-blip-captions"

accelerate launch train_text_to_image.py \
  --pretrained_model_name_or_path=$MODEL_NAME \
  --dataset_name=$dataset_name \
  --use_ema \
  --resolution=512 --center_crop --random_flip \
  --train_batch_size=1 \
  --gradient_accumulation_steps=4 \
  --gradient_checkpointing \
  --mixed_precision="fp16" \
  --max_train_steps=15000 \
  --learning_rate=1e-05 \
  --max_grad_norm=1 \
  --lr_scheduler="constant" --lr_warmup_steps=0 \
  --output_dir="sd-pokemon-model" 
```

자체 학습 파일에서 실행하려면 `datasets`에 필요한 형식에 따라 데이터셋을 준비해야 합니다. 데이터셋을 허브에 업로드하거나 파일이 있는 로컬 폴더를 준비할 수 있습니다. [이 문서](https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder-with-metadata)에서 이를 수행하는 방법을 설명합니다.

커스텀 로딩 로직을 사용하려면 스크립트를 수정해야 합니다. 코드의 적절한 위치에 포인터를 남겼습니다 :)

```bash
export MODEL_NAME="CompVis/stable-diffusion-v1-4"
export TRAIN_DIR="path_to_your_dataset"
export OUTPUT_DIR="path_to_save_model"

accelerate launch train_text_to_image.py \
  --pretrained_model_name_or_path=$MODEL_NAME \
  --train_data_dir=$TRAIN_DIR \
  --use_ema \
  --resolution=512 --center_crop --random_flip \
  --train_batch_size=1 \
  --gradient_accumulation_steps=4 \
  --gradient_checkpointing \
  --mixed_precision="fp16" \
  --max_train_steps=15000 \
  --learning_rate=1e-05 \
  --max_grad_norm=1 \
  --lr_scheduler="constant" --lr_warmup_steps=0 \
  --output_dir=${OUTPUT_DIR}
```

학습이 완료되면 모델은 명령에 지정된 `OUTPUT_DIR`에 저장됩니다. 추론을 위해 미세 조정된 모델을 로드하려면, 해당 경로를 `StableDiffusionPipeline`으로 전달하기만 하면 됩니다:

```python
from diffusers import StableDiffusionPipeline

model_path = "path_to_saved_model"
pipe = StableDiffusionPipeline.from_pretrained(model_path, torch_dtype=torch.float16)
pipe.to("cuda")

image = pipe(prompt="yoda").images[0]
image.save("yoda-pokemon.png")
```

### Flax / JAX 미세 조정

[@duongna211](https://github.com/duongna21) 덕분에 Flax를 사용하여 Stable Diffusion을 미세 조정할 수 있습니다! 이는 TPU 하드웨어에서 매우 효율적이지만 GPU에서도 훌륭하게 작동합니다. 다음과 같이 [Flax 학습 스크립트](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_flax.py)를 사용할 수 있습니다:

```Python
export MODEL_NAME="runwayml/stable-diffusion-v1-5"
export dataset_name="lambdalabs/pokemon-blip-captions"

python train_text_to_image_flax.py \
  --pretrained_model_name_or_path=$MODEL_NAME \
  --dataset_name=$dataset_name \
  --resolution=512 --center_crop --random_flip \
  --train_batch_size=1 \
  --max_train_steps=15000 \
  --learning_rate=1e-05 \
  --max_grad_norm=1 \
  --output_dir="sd-pokemon-model" 
```
