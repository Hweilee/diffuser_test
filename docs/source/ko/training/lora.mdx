<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Diffusers에서의 LoRA 지원

Diffusers는 Stable Diffusion의 더 빠른 미세 조정을 위해 LoRA를 지원하여 더 큰 메모리 효율성과 더 쉬운 휴대성을 가지게 해줍니다.

Large Language Models의 Low-Rank Adaption는 Microsoft에 의해 *Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen*에 의한[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)에서 소개되었습니다.

간단히 말해서, LoRA는 기존 가중치에 순위 분해 가중치 행렬 쌍(**업데이트 행렬**이라고 함)을 추가하고 새로 추가된 가중치**만** 훈련함으로써 사전 훈련된 모델을 적용할 수 있습니다. 여기에는 몇 가지 장점이 있습니다.

- 이전에 사전 훈련된 가중치는 고정된 상태로 유지되어 모델이 [치명적 망각](https://www.pnas.org/doi/10.1073/pnas.1611835114) 경향이 없습니다.
- 순위 분해 행렬은 원래 모델보다 파라메터 수가 훨씬 적으므로 훈련된 LoRA 가중치를 쉽게 이식할 수 있습니다.
- LoRA 행렬들은 일반적으로 원본 모델의 주의 레이어에 추가되며 `scale` 매개변수를 통해 모델이 새로운 학습 이미지에 적용되는 정도를 제어합니다.

**__LoRA의 사용이 어텐션 레이어에만 국한되지 않는 점을 참고하세요. 원래 LoRA 작업에서는, 저자들은 언어 모델의 어텐션 레이어를 수정하는 것만으로도 매우 효율적으로 우수한 다운스트림 성능을 얻기에 충분하다는 것을 알아냈습니다. 이것이 LoRA 가중치를 모델의 어텐션 레이어에 추가하는 것이 일반적인 이유입니다.__**

[cloneofsimo](https://github.com/cloneofsimo)가 인기 있는 [lora](https://github.com/cloneofsimo/lora) GitHub 리포지토리에서, Stable Diffusion에 대해 LoRA 학습을 최초로 시도했습니다.

<Tip>

LoRA를 사용하면 사전 훈련된 가중치가 고정되고 LoRA 가중치만 학습되므로 Tesla T4, RTX 3080 또는 심지어 RTX 2080 Ti와 같은 소비자 GPU에서 미세 조정을 실행할 수 있으므로 더 큰 메모리 효율성을 달성할 수 있습니다! Kaggle Kernels 및 Google Colab Notebooks의 무료 등급에서 T4와 같은 GPU에 액세스할 수 있습니다.

</Tip>

## 미세 조정을 위해 LoRA 시작하기

Stable Diffusion은 다양한 방법으로 미세 조정할 수 있습니다:

* [Textual inversion](https://huggingface.co/docs/diffusers/main/kr/training/text_inversion)
* [DreamBooth](https://huggingface.co/docs/diffusers/main/kr/training/dreambooth) 
* [Text2Image 미세 조정](https://huggingface.co/docs/diffusers/main/kr/training/text2image) 

LoRA로 미세 조정을 실행하는 방법을 보여주는 두 가지 종단 간 예시를 제공합니다:

* [DreamBooth](https://github.com/huggingface/diffusers/tree/main/examples/dreambooth#training-with-low-rank-adaptation-of-large-language-models-lora) 
* [Text2Image](https://github.com/huggingface/diffusers/tree/main/examples/text_to_image#training-with-lora)

예를 들어 LoRA로 DreamBooth 학습을 수행하려면 다음을 실행합니다:

```bash
export MODEL_NAME="runwayml/stable-diffusion-v1-5"
export INSTANCE_DIR="path-to-instance-images"
export OUTPUT_DIR="path-to-save-model"

accelerate launch train_dreambooth_lora.py \
  --pretrained_model_name_or_path=$MODEL_NAME  \
  --instance_data_dir=$INSTANCE_DIR \
  --output_dir=$OUTPUT_DIR \
  --instance_prompt="a photo of sks dog" \
  --resolution=512 \
  --train_batch_size=1 \
  --gradient_accumulation_steps=1 \
  --checkpointing_steps=100 \
  --learning_rate=1e-4 \
  --report_to="wandb" \
  --lr_scheduler="constant" \
  --lr_warmup_steps=0 \
  --max_train_steps=500 \
  --validation_prompt="A photo of sks dog in a bucket" \
  --validation_epochs=50 \
  --seed="0" \
  --push_to_hub
```

`examples/text_to_image/train_text_to_image_lora.py` 스크립트를 사용하여 커스텀 데이터셋에서 Stable Diffusion을 완전히 미세 조정하기 위해 유사한 프로세스를 따를 수 있습니다.

자세한 내용은 위에 링크된 각 예시를 참조하십시오.

<Tip>

LoRA를 사용할 때 LoRA가 아닌 Dreambooth 미세 조정에 비해 훨씬 더 높은 학습률(일반적으로 ~1e-6이 아닌 1e-4)을 사용할 수 있습니다.

</Tip>

그러나 공짜 점심은 없습니다. 주어진 데이터셋과 예상되는 생성 품질에 대해 여전히 다른 하이퍼파라미터로 실험해야 합니다. 다음은 몇 가지 중요한 사항입니다:

* 학습 시간
    * 학습률
    * 학습 step 수
* 추론 시간
    * steps 수
    * 스케줄러 종류

또한 Stable Diffusion의 DreamBooth 학습을 수행하기 위한 실험 결과 문서를 [이 블로그](https://huggingface.co/blog/dreambooth)로 팔로우할 수 있습니다.

미세 조정 시, LoRA 업데이트 행렬은 어텐션 레이어들에만 추가됩니다. 이를 활성하시키기 위해, 새로운 가중치 로딩 기능을 추가했습니다. 자세한 내용은 [여기](https://huggingface.co/docs/diffusers/main/en/api/loaders)에서 확인할 수 있습니다.

## 추론

[Pokemon 데이터셋](https://huggingface.co/datasets/lambdalabs/pokemon-blip-captions)으로 Stable Diffusion을 미세 조정하기 위해 `examples/text_to_image/train_text_to_image_lora.py`를 사용할 때, 다음과 같은 추론을 수행할 수 있습니다:

```py 
from diffusers import StableDiffusionPipeline
import torch

model_path = "sayakpaul/sd-model-finetuned-lora-t4"
pipe = StableDiffusionPipeline.from_pretrained("CompVis/stable-diffusion-v1-4", torch_dtype=torch.float16)
pipe.unet.load_attn_procs(model_path)
pipe.to("cuda")

prompt = "A pokemon with blue eyes."
image = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]
image.save("pokemon.png")
```

다음은 기대되는 몇 가지 예시 이미지입니다.

<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pokemon-collage.png"/>

[`sayakpaul/sd-model-finetuned-lora-t4`](https://huggingface.co/sayakpaul/sd-model-finetuned-lora-t4)에는 [LoRA 미세 조정 업데이트 행렬](https:// huggingface.co/sayakpaul/sd-model-finetuned-lora-t4/blob/main/pytorch_lora_weights.bin)을 포함하며, 이 크기는 3MB에 불과합니다.
추론하는 동안, 사전 훈련된 Stable Diffusion 체크포인트는 이러한 업데이트 행렬과 함께 로드된 다음 결합되어 추론을 실행합니다.

[`sayakpaul/sd-model-finetuned-lora-t4`](https://huggingface.co/sayakpaul/sd-model-finetuned-lora-t4)을 차지 위해 [`huggingface_hub`](https://github.com/huggingface/huggingface_hub) 라이브러리를 다음과 같이 사용할 수 있습니다:

```py
from huggingface_hub.repocard import RepoCard

card = RepoCard.load("sayakpaul/sd-model-finetuned-lora-t4")
base_model = card.data.to_dict()["base_model"]
# 'CompVis/stable-diffusion-v1-4'
```

그리고 `pipe = StableDiffusionPipeline.from_pretrained(base_model, torch_dtype=torch.float16)`로 사용할 수 있습니다.

이는 `StableDiffusionPipeline`을 초기화하는 동안 기본 모델 식별자를 하드코딩하지 않으려는 경우에 특히 유용합니다.

DreamBooth 학습 결과에 대한 추론은 동일합니다. [이 섹션](https://github.com/huggingface/diffusers/tree/main/examples/dreambooth#inference-1)에서 자세한 내용을 확인하세요.

### 원본 모델과 LoRA 합치기

추론을 수행할 때, 훈련된 LoRA 가중치를 고정된 사전 훈련된 모델 가중치와 병합하여 원래 모델의 추론 결과(미세 조정이 발생하지 않은 것처럼)와 완전히 미세 조정된 버전에 대해 보간할 수 있습니다.

논문에서는 α(알파), 구현된 코드에서는 `scale`이라는 매개변수를 사용하여 병합 비율을 조정할 수 있습니다. 파이프라인 호출에서 `scale`을 `cross_attention_kwargs`로 전달하는 방식으로 코드를 작성할 수 있습니다.

```py 
from diffusers import StableDiffusionPipeline
import torch

model_path = "sayakpaul/sd-model-finetuned-lora-t4"
pipe = StableDiffusionPipeline.from_pretrained("CompVis/stable-diffusion-v1-4", torch_dtype=torch.float16)
pipe.unet.load_attn_procs(model_path)
pipe.to("cuda")

prompt = "A pokemon with blue eyes."
image = pipe(prompt, num_inference_steps=30, guidance_scale=7.5, cross_attention_kwargs={"scale": 0.5}).images[0]
image.save("pokemon.png")
```

`0`은 LoRA 가중치를 사용하지 _않는_ 것과 같고 `1`은 LoRA 미세 조정된 가중치만 사용함을 의미합니다. 0과 1 사이의 값은 두 결과들 사이로 보간됩니다.


## 알려진 제한들

* 현재 [`UNet2DConditionModel`](https://huggingface.co/docs/diffusers/main/en/api/models#diffusers.UNet2DConditionModel)의 어텐션 레이어에 대해서만 LoRA를 지원합니다.