<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Controlled generation

การควบคุมผลลัพธ์ที่สร้างโดยโมเดล diffusion  (diffusion models) เป็นเป้าหมายที่ความต้องการมาตราฐานในชุมชนนี้มานานแล้วและเป็นหัวข้อการวิจัยที่กำลังพัฒนาอย่างต่อเนื่องในปัจจุบัน ในโมเดล diffusion ที่ได้รับความนิยมมีหลายรูปแบบที่การเปลี่ยนแปลงละเอียดเฉียบในข้อมูลของขาเข้าทั้งภาพและข้อความสามารถทำให้ผลลัพธ์เปลี่ยนไปอย่างมีชัดเจน ในโลกที่เป็นไปตามความคาดหวัง เราต้องการที่จะสามารถควบคุมได้ว่าความหมายถูกสงวนไว้และเปลี่ยนแปลงอย่างไร

ส่วนมากของตัวอย่างการสงวนความหมายลดลงเป็นการสามารถแมปการเปลี่ยนแปลงของข้อมูลขาเข้าในการเปลี่ยนแปลงของผลลัพธ์ได้อย่างแม่นยำ เช่น เพิ่มคำคุณศัพท์ในเรื่องที่เป็นกระบอก noise ยังคงรักษาภาพรวมของภาพ โดยเปลี่ยนแปลงเฉพาะกระบอก noise ที่เปลี่ยนแปลง หรือ การแปรผันภาพของกระบอก noise ในท่าทางบางอย่างยังคงรักษาท่าทางของกระบอก noise นั้น

นอกจากนี้ยังมีคุณลักษณะของภาพที่เราต้องการที่จะมีผลต่อเป็นไปในการสร้างภาพที่เราต้องการเสมอ ตัวอย่างเช่น ทั่วไปเราต้องการผลลัพธ์ที่มีคุณภาพดี ทันสไตล์ที่เฉพาะเจาะจง หรือสมจริง

เราจะระบุบางเทคนิคที่ `diffusers` รองรับเพื่อควบคุมการสร้างของโมเดล diffusion  หลายอย่างเป็นการวิจัยของมุมมองที่หลังหลายและสามารถเป็นได้เป็นการหลงเป็นรายละเอียด หากมีข้อสงสัยหรือมีข้อเสนอแนะ อย่าลังเลที่จะเปิดการสนทนาใน [forum](https://discuss.huggingface.co/) หรือ [GitHub issue](https://github.com/huggingface/diffusers/issues)

เราจะให้คำอธิบายระดับสูงเกี่ยวกับวิธีการควบคุมการสร้างรวมถึงตัวอย่างโปรโตไทป์เทคนิคบางประการ เพื่อข้อมูลที่ละเอียดมากเกี่ยวกับเทคนิค เอกสารต้นฉบับที่ลิงก์จากท่อนโปรโตไทป์เสมอเป็นทรัพยากรที่ดีที่สุด

ขึ้นอยู่กับกรณีการใช้งาน คนสามารถเลือกใช้เทคนิคต่าง ๆ ตามความเหมาะสม ในหลายกรณี สามารถรวมเทคนิคเหล่านี้ได้ เช่น สามารถรวม Textual Inversion กับ SEGA เพื่อให้การนำทางเชิงความหมายมากขึ้นสำหรับผลลัพธ์ที่สร้างโดย Textual Inversion

นอกเหนือจากนี้ ถ้าไม่ได้ระบุไว้เป็นอื่น ๆ นี้เป็นเทคนิคที่ทำงานกับโมเดลที่มีอยู่และไม่ต้องใช้น้ำหนักของตัวเอง

1. [Instruct Pix2Pix](#instruct-pix2pix)
2. [Pix2Pix Zero](#pix2pixzero)
3. [Attend and Excite](#attend-and-excite)
4. [Semantic Guidance](#semantic-guidance)
5. [Self-attention Guidance](#self-attention-guidance)
6. [Depth2Image](#depth2image)
7. [MultiDiffusion Panorama](#multidiffusion-panorama)
8. [DreamBooth](#dreambooth)
9. [Textual Inversion](#textual-inversion)
10. [ControlNet](#controlnet)
11. [Prompt Weighting](#prompt-weighting)
12. [Custom Diffusion](#custom-diffusion)
13. [Model Editing](#model-editing)
14. [DiffEdit](#diffedit)
15. [T2I-Adapter](#t2i-adapter)
16. [Fabric](#fabric)

เพื่อความสะดวก เราจะให้ตารางเพื่อระบุว่าวิธีการไหนเป็นการนำไปใช้เฉพาะในขั้นตอน inference เท่านั้น และวิธีการไหนต้องการการฝึก/การปรับแต่ง

|                     **วิธีการ**                      | **Inference only** | **Requires training /<br> fine-tuning** |                                          **Comments**                                           |
| :-------------------------------------------------: | :----------------: | :-------------------------------------: | :---------------------------------------------------------------------------------------------: |
|        [Instruct Pix2Pix](#instruct-pix2pix)        |         ✅         |                   ❌                    | สามารถปรับแต่งเพิ่มเติมได้<br>เพื่อประสิทธิภาพที่ดีขึ้นต่อ<br>คำแนะนำการแก้ไขที่เฉพาะเจาะจง |
|            [Pix2Pix Zero](#pix2pixzero)             |         ✅         |                   ❌                    |                                                                                                 |
|       [Attend and Excite](#attend-and-excite)       |         ✅         |                   ❌                    |                                                                                                 |
|       [Semantic Guidance](#semantic-guidance)       |         ✅         |                   ❌                    |                                                                                                 |
| [Self-attention Guidance](#self-attention-guidance) |         ✅         |                   ❌                    |                                                                                                 |
|             [Depth2Image](#depth2image)             |         ✅         |                   ❌                    |                                                                                                 |
| [MultiDiffusion Panorama](#multidiffusion-panorama) |         ✅         |                   ❌                    |                                                                                                 |
|              [DreamBooth](#dreambooth)              |         ❌         |                   ✅                    |                                                                                                 |
|       [Textual Inversion](#textual-inversion)       |         ❌         |                   ✅                    |                                                                                                 |
|              [ControlNet](#controlnet)              |         ✅         |                   ❌                    | ControlNet สามารถถูกฝึก/ปรับแต่งเพื่อ<br>ตรงตามเงื่อนไขที่กำหนดเองได้                |
|        [Prompt Weighting](#prompt-weighting)        |         ✅         |                   ❌                    |                                                                                                 |
|        [Custom Diffusion](#custom-diffusion)        |         ❌         |                   ✅                    |                                                                                                 |
|           [Model Editing](#model-editing)           |         ✅         |                   ❌                    |                                                                                                 |
|                [DiffEdit](#diffedit)                |         ✅         |                   ❌                    |                                                                                                 |
|             [T2I-Adapter](#t2i-adapter)             |         ✅         |                   ❌                    |                                                                                                 |
|                [Fabric](#fabric)                    |         ✅         |                   ❌                    |                                                                                                 |

## Instruct Pix2Pix

[Paper](https://arxiv.org/abs/2211.09800)

[Instruct Pix2Pix](../api/pipelines/pix2pix) ได้รับการปรับแต่งจาก diffusion ที่เสถียรเพื่อรองรับการแก้ไขภาพขาเข้า มีภาพและคำแนะนำเพื่ออธิบายการแก้ไขเป็นข้อมูลขาเข้าและผลลัพธ์ที่แก้ไขเป็นผลลัพธ์ออกมา
Instruct Pix2Pix ได้รับการฝึกอย่างชัดเจนเพื่อทำงานได้ดีกับคำแนะนำที่คล้ายกับ [InstructGPT](https://openai.com/blog/instruction-following/)

ดูเพิ่มเติม[ที่นี่](../api/pipelines/pix2pix) เพื่อข้อมูลเพิ่มเติมเกี่ยวกับวิธีการใช้

## Pix2Pix Zero

[Paper](https://arxiv.org/abs/2302.03027)

[Pix2Pix Zero](../api/pipelines/pix2pix_zero) ช่วยให้สามารถแก้ไขภาพโดยที่หนึ่งความคิดหรือเรื่องอื่นๆ ถูกแปลงเป็นอีกอย่างในขณะที่รักษาความหมายของภาพโดยรวม
กระบวนการลด noise ระบายไปยังกระบวนการลด noise อื่น ๆ ถูกนำมาชี้แนะต่อเนื่องระหว่างการลด noise โดยใช้เส้นทางความสนใจอย่างต่ำ
และสร้างแรงจูงในแต่ละตัวแปรกลางในระหว่างกระบวนการลด noise เพื่อดันแผนที่ความสนใจไปที่แผนที่ความสนใจของการลด noise อ้างอิง
แผนที่ความสนใจอ้างอิงมาจากกระบวนการลด noise ของภาพขาเข้าและถูกใช้เพื่อกระตุ้นการรักษาความหมาย

Pix2Pix Zero สามารถใช้แก้ไขภาพสังเสือนเช่นเดียวกับภาพเสมือน
- เพื่อแก้ไขภาพสังเสือน แรกจะต้องสร้างภาพจากคำบรรยาย
  ต่อมา เราสร้างคำบรรยายของแนวคิดที่จะแก้ไขและสำหรับแนวคิดเป้าหมายใหม่ เราสามารถใช้โมเดลเช่น [Flan-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5) สำหรับวัตถุประสงค์นี้ได้ จากนั้น "mean" prompt embeddings สำหรับทั้งแนวคิดต้นฉบับและแนวคิดเป้าหมายจะถูกสร้างผ่านตัวเข้ารหัสข้อความ สุดท้าย จะใช้วิธีการ pix2pix-zero เพื่อแก้ไขภาพเสมือน
- เพื่อแก้ไขภาพจริง แรกจะต้องสร้างคำบรรยายภาพโดยใช้โมเดลเช่น [BLIP](https://huggingface.co/docs/transformers/model_doc/blip) จากนั้น ให้ใช้การพลิกคำบรรยายและภาพเพื่อสร้างตัวแปร "inverse" latents ตามที่เคยกล่าวถึง สุดท้าย "mean" prompt embeddings สำหรับทั้งแนวคิดต้นฉบับและแนวคิดเป้าหมายจะถูกสร้างและในที่สุด ใช้วิธีการ pix2pix-zero ร่วมกับตัวแปร "inverse" เพื่อแก้ไขภาพ

<Tips>

Pix2Pix Zero คือโมเดลแรกที่ช่วยแก้ไขภาพ "ซีโร่" ซึ่งหมายความว่าโมเดลสามารถ
แก้ไขภาพในไม่กี่นาทีใน GPU ของผู้บริโภค เช่นที่แสดงไว้ [ที่นี่](../api/pipelines/pix2pix_zero#usage-example)

</Tips>

เหมือนกับที่กล่าวไว้ข้างบน Pix2Pix Zero รวมถึงการปรับแต่งตัวแปร (และไม่ได้ปรับแต่ง UNet, VAE หรือตัวเข้ารหัสข้อความ) เพื่อนำทางกระบวนการสร้างไปสู่แนวคิดที่กำหนดไว้ นั่นหมายความว่าทั้ง
ทั้งกระบวนการอาจต้องการหน่วยความจำมากกว่า [StableDiffusionPipeline](../api/pipelines/stable_diffusion/text2img) ทั่วๆ ไป

ดูเพิ่มเติม[ที่นี่](../api/pipelines/pix2pix_zero) เพื่อข้อมูลเพิ่มเติมเกี่ยวกับวิธีการใช้

## Attend and Excite

[Paper](https://arxiv.org/abs/2301.13826)

[Attend and Excite](../api/pipelines/attend_and_excite) ช่วยให้ผู้ตรวจสอบในคำแนะนำถูกแสดงในภาพที่สมบูรณ์

กำหนดชุดดัชนีโทเค็นเป็นข้อมูลขาเข้าที่เกี่ยวข้องกับกระทู้ที่ต้องอยู่ในภาพ ในระหว่างการลด noise  แต่ละดัชนีโทเค็นมีการรับรองว่าจะมีค่าเข้มที่ต่ำที่สุดสำหรับอย่างน้อยหนึ่งแพทช์ของภาพ ตลอดกระบวนการลด noise  ตัวแปรกลางได้รับการปรับแต่งต่อเนื่องในระหว่างกระบวนการลด noise เพื่อให้มีค่าเข้มที่สูงขึ้นเมื่อมีคำแนะนำที่เกี่ยวข้อง

ดูเพิ่มเติม[ที่นี่](../api/pipelines/attend_and_excite) เพื่อข้อมูลเพิ่มเติมเกี่ยวกับวิธีการใช้

## Semantic Guidance

[Paper](https://arxiv.org/abs/2301.13826)

[Semantic Guidance](../api/pipelines/semantic_guidance) ช่วยให้ผู้ตรวจสอบในคำแนะนำถูกแสดงในภาพที่สมบูรณ์

แตกต่างจาก `Attend and Excite` ที่ใช้การรับรองด้วยเส้นทางน้ำหนักควบคุมที่มีประสิทธิภาพในการเพิ่มความสำคัญของแพทช์ในภาพ สำหรับ `Semantic Guidance` เราใช้การแก้ไขข้อมูลขาเข้าเพื่อเพิ่มความสำคัญของส่วนของภาพที่สนใจ

แรกเราสร้างคำแนะนำสำหรับการแก้ไขในท่าทางของแทร็กที่ไม่จำเป็น แต่ที่มีผลต่อความหมายของภาพทั้งหมด จากนั้นเราสร้างแผนที่ความสนใจของคำแนะนำนั้น และนำไปใช้ในการกระตุ้นการแก้ไขข้อมูลขาเข้า เพื่อเน้นความสำคัญของส่วนของภาพที่สอดคล้องกับคำแนะนำนั้น

ดูเพิ่มเติม[ที่นี่](../api/pipelines/semantic_guidance) เพื่อข้อมูลเพิ่มเติมเกี่ยวกับวิธีการใช้

## Self-attention Guidance

[Paper](https://arxiv.org/abs/2301.13826)

[Self-attention Guidance](../api/pipelines/self_attention_guidance) ช่วยให้ผู้ตรวจสอบในคำแนะนำถูกแสดงในภาพที่สมบูรณ์

เปรียบเทียบกับ `Attend and Excite` ที่ใช้การเพิ่มความสำคัญของแพทช์ด้วยการทำให้มีค่าเข้มสูงขึ้น ทำให้มีส่วนที่สนใจมากขึ้นในภาพ สำหรับ `Self-attention Guidance` เราใช้การแก้ไขข้อมูลขาเข้าเพื่อเพิ่มความสำคัญของการใช้คำแนะนำที่ต่างกัน

แรกเราใช้การใส่ส่วนจำกัดของข้อมูลขาเข้าที่ต้องการให้มีผลต่อคำแนะนำที่ให้เป้าหมายเป็นค่าใหม่ที่ส่วนใหญ่เป็นศูนย์ จากนั้นนำผลลัพธ์ไปใช้ในการกระตุ้นค่าการควบคุมสำหรับส่วนของภาพที่ตรงกับข้อมูลขาเข้าที่ถูกแก้ไข ทำให้คำแนะนำมีผลต่อภาพที่สนใจ

ดูเพิ่มเติม[ที่นี่](../api/pipelines/self_attention_guidance) เพื่อข้อมูลเพิ่มเติมเกี่ยวกับวิธีการใช้

## Depth2Image

[Paper](https://arxiv.org/abs/2302.03027)

[Depth2Image](../api/pipelines/depth2image) ช่วยให้ผู้ตรวจสอบในคำแนะนำถูกแสดงในภาพที่สมบูรณ์

เราสร้างคำแนะนำเพื่อให้ภาพที่มีความลึกเป็นผลลัพธ์ โดยรวมทั้งรูปภาพและคำแนะนำเพื่ออธิบายความลึก สำหรับกระบวนการลด noise  เราใช้การแก้ไขข้อมูลขาเข้าเพื่อเพิ่มความสำคัญของลำดับคำแนะนำที่ทำให้รูปภาพมีความลึก

การนำเข้าภาพและคำแนะนำจะถูกทำให้เหมือนกับว่าคำแนะนำกำหนดความลึกของภาพ ซึ่งจะถูกใช้ในการกระตุ้นการแก้ไขข้อมูลขาเข้า โดยให้ความสำคัญสูงของลำดับคำแนะนำนั้น ๆ

ดูเพิ่มเติม[ที่นี่](../api/pipelines/depth2image) เพื่อข้อมูลเพิ่มเติมเกี่ยวกับวิธีการใช้

## MultiDiffusion Panorama

[Paper](https://arxiv.org/abs/2302.03027)

[MultiDiffusion Panorama](../api/pipelines/multidiffusion_panorama) ช่วยให้ผู้ตรวจสอบในคำแนะนำถูกแสดงในภาพที่สมบูรณ์

กระบวนการลด noise ทำงานบนภาพที่มีขนาดใหญ่กว่า (เช่น รูปภาพทิ้งท้าย) โดยใช้หลายเส้นทางลด noise พร้อมกัน
แต่ละเส้นทางลด noise ได้รับการควบคุมด้วยตัวแปรกลางขึ้นอยู่กับกระบวนการและตำแหน่งที่ให้คำแนะนำในภาพ

ในการลด noise แต่ละครั้ง คำแนะนำทั้งหมดจะถูกใช้ในการกระตุ้นตัวแปรกลางและนำไปสู่ผลลัพธ์ของกระบวนการ
ทำให้กระบวนการและตำแหน่งของคำแนะนำมีผลต่อภาพผลลัพธ์ที่ได้

ดูเพิ่มเติม[ที่นี่](../api/pipelines/multidiffusion_panorama) เพื่อข้อมูลเพิ่มเติมเกี่ยวกับวิธีการใช้

## DreamBooth

[Paper](https://arxiv.org/abs/2302.03027)

[DreamBooth](../api/pipelines/dreambooth) เป็นกระบวนการที่ช่วยในการแก้ไขภาพที่สามารถนำไปใช้ใน DreamBooth

เราสร้างคำแนะนำที่เกี่ยวข้องกับการแก้ไขที่ควรทำกับภาพที่สมบูรณ์ใน DreamBooth
ซึ่งเป็นกระบวนการที่ช่วยในการเพิ่มความเป็นฝันในรูปภาพ สำหรับกระบวนการลด noise  เราใช้การแก้ไขข้อมูลขาเข้าเพื่อเพิ่มความสำคัญของการใช้คำแนะนำที่เข้าข่ายในภาพ

การนำเข้าภาพและคำแนะนำจะถูกให้เหมือนกับว่าคำแนะนำกำหนดการแก้ไขที่สร้างภาพที่เป็นฝัน ซึ่งจะถูกใช้ในการกระตุ้นการแก้ไขข้อมูลขาเข้า โดยให้ความสำคัญสูงของคำแนะนำที่เข้าข่าย

ดูเพิ่มเติม[ที่นี่](../api/pipelines/dreambooth) เพื่อข้อมูลเพิ่มเติมเกี่ยวกับวิธีการใช้

## Textual Inversion

[Paper](https://arxiv.org/abs/2302.03027)

[Textual Inversion](../api/pipelines/textual_inversion) ช่วยให้ผู้ตรวจสอบในคำแนะนำถูกแสดงในภาพที่สมบูรณ์

การเริ่มต้นจากคำแนะนำที่เกี่ยวข้องกับภาพที่ต้องการแก้ไข เราสร้างคำแนะนำเพื่อให้ภาพผลลัพธ์เป็นภาพที่สนใจ สำหรับกระบวนการลด noise  เราใช้การแก้ไขข้อมูลขาเข้าเพื่อเพิ่มความสำคัญของการใช้คำแนะนำนั้น ๆ

การนำเข้าภาพและคำแนะนำจะถูกให้เหมือนกับว่าคำแนะนำกำหนดภาพที่สนใจ ซึ่งจะถูกใช้ในการกระตุ้นการแก้ไขข้อมูลขาเข้า โดยให้ความสำคัญสูงของคำแนะนำ

ดูเพิ่มเติม[ที่นี่](../api/pipelines/textual_inversion) เพื่อข้อมูลเพิ่มเติมเกี่ยวกับวิธีการใช้

## Captcha

[Paper](https://arxiv.org/abs/2302.03027)

[Captcha](../api/pipelines/captcha) ช่วยให้ผู้ตรวจสอบในคำแนะนำถูกแสดงในภาพที่สมบูรณ์

การเริ่มต้นจากคำแนะนำที่เกี่ยวข้องกับภาพที่ต้องการแก้ไข เราสร้างคำแนะนำเพื่อให้ภาพผลลัพธ์เป็นภาพที่สนใจ สำหรับกระบวนการลด noise  เราใช้การแก้ไขข้อมูลขาเข้าเพื่อเพิ่มความสำคัญของการใช้คำแนะนำนั้น ๆ

การนำเข้าภาพและคำแนะนำจะถูกให้เหมือนกับว่าคำแนะนำกำหนดภาพที่สนใจ ซึ่งจะถูกใช้ในการกระตุ้นการแก้ไขข้อมูลขาเข้า โดยให้ความสำคัญสูงของคำแนะนำ

ดูเพิ่มเติม[ที่นี่](../api/pipelines/captcha) เพื่อข้อมูลเพิ่มเติมเกี่ยวกับวิธีการใช้