<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Torch2.0 support in diffusers.

Starting from version `0.13.0`, Diffusers supports the latest optimization from the upcoming [PyTorch 2.0](https://pytorch.org/get-started/pytorch-2.0/) release. These include:
1. Support for native flash and memory-efficient attention (from xFormers) without any extra dependencies.
2. [`torch.compile`](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) support for compiling individual models for extra performance boost.


## Installation
To benefit from the native efficient attention and `torch.compile`, we will need to install the nightly version of PyTorch as the stable version is yet to be released. The first step is to install CUDA11.7, as torch2.0 does not support the previous version. Once CUDA11.7 is installed, 
torch nightly can be installed using 

```bash
pip install --pre torch torchvision --index-url https://download.pytorch.org/whl/nightly/cu117
```

## Using efficient attention and torch.compile.


1. **Efficient Attention**

    The efficient attention is implemented via the `torch.nn.functional.scaled_dot_product_attention` function, which automatically enables flash/memory efficient attention, depending on the input and the GPU type. This is the same as the `memory_efficient_attention` from [xFormers](https://github.com/facebookresearch/xformers) but built natively into the torch. For more information, refer to torch [docs](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention).

    The efficient attention will be enabled by default if torch2.0 is installed and if `torch.nn.functional.scaled_dot_product_attention` is available. To use it, you can just install torch2.0 as suggested above and use the pipeline. For example:

    ```Python
    import torch
    from diffusers import StableDiffusionPipeline

    pipe = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16)
    pipe = pipe.to("cuda")

    prompt = "a photo of an astronaut riding a horse on mars"
    image = pipe(prompt).images[0]
    ```

    If you want to enable it explicitly (which is not required), you can do so as shown below.

    ```Python
    import torch
    from diffusers import StableDiffusionPipeline
    from diffusers.models.cross_attention import Torch2AttnProcessor

    pipe = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16).to("cuda")
    pipe.unet.set_attn_processor(Torch2AttnProcessor())

    prompt = "a photo of an astronaut riding a horse on mars"
    image = pipe(prompt).images[0]
    ```

    This should be as fast and memory efficient as `xFormers`.


2. **torch.compile**

    To get further speed up, we can use the new `torch.compile` feature. To do so, we wrap our `unet` with `torch.compile`. For more information and different options, refer to the 
    [torch compile docs](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html).

    ```python
    import torch
    from diffusers import StableDiffusionPipeline

    pipe = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, safety_checker=None).to("cuda")
    pipe.unet = torch.compile(pipe.unet)

    batch_size = 10
    prompt =  "A photo of an astronaut riding a horse on marse."
    images = pipe(prompt,  num_inference_steps=steps, num_images_per_prompt=batch_size).images
    ```

    Depending on the type of GPUs it can give between 2-9% spee-up over efficient attention. But note that as of now the speed-up is mostly noticable on high-end GPUs such as A100.