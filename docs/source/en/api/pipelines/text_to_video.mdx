<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Text-to-video synthesis

Text-to-video synthesis from [ModelScope](https://modelscope.cn/) can be considered the same as Stable Diffusion structure-wise but it is extended to videos instead of static images. More specifically, this system allows us to generate videos from a natural language text prompt.

From the [model summary](https://huggingface.co/damo-vilab/modelscope-damo-text-to-video-synthesis):

*This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.*

Resources:

* [Website](https://modelscope.cn/models/damo/text-to-video-synthesis/summary)
* [GitHub repository](https://github.com/modelscope/modelscope/)
* [Spaces] (TODO)

## Available Pipelines:

| Pipeline | Tasks | Demo
|---|---|:---:|
| [TextToVideoMSPipeline](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py) | *Text-to-Video Generation* | [Spaces] (TODO)

## Usage example 

Let's start by generating a short video:

```python 
import torch
from diffusers import TextToVideoMSPipeline
from diffusers.utils import export_to_video

pipe = TextToVideoMSPipeline.from_pretrained("diffusers/ms-text-to-video-1.7b", torch_dtype=torch.float16)
pipe = pipe.to("cuda")

prompt = "Spiderman is surfing"
video_frames = pipe(prompt).frames
video_path = export_to_video(video_frames)
video_path
```

Diffusers supports different optimization techniques to for improving the latency
and memory footprint of a pipeline. Since videos are often more memory-heavy than images,
for this pipeline, we can enable CPU offloading and VAE slicing to keep the memory-footprint at bay.

Let's generate a video of 8 seconds with CPU offloading and VAE slicing:

```python
import torch
from diffusers import TextToVideoMSPipeline
from diffusers.utils import export_to_video

pipe = TextToVideoMSPipeline.from_pretrained("diffusers/ms-text-to-video-1.7b", torch_dtype=torch.float16)
pipe = pipe.to("cuda")

# memory optimization
pipe.enable_model_cpu_offload()
pipe.enable_vae_slicing()

prompt = "Darth Vader surfing a wave"
video_frames = pipe(prompt, num_frames=64, num_inference_steps=25).frames
video_path = export_to_video(video_frames)
video_path
```

Together with PyTorch 2.0, "fp16" as the precision and the above techniques, it just takes 7 GBs of GPU memory.

We can also use a different scheduler easily:

```python
import torch
from diffusers import TextToVideoMSPipeline, DPMSolverMultistepScheduler
from diffusers.utils import export_to_video

pipe = TextToVideoMSPipeline.from_pretrained("diffusers/ms-text-to-video-1.7b", torch_dtype=torch.float16)
pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
pipe = pipe.to("cuda")

prompt = "Spiderman is surfing"
video_frames = pipe(prompt).frames
video_path = export_to_video(video_frames)
video_path
```

## Available checkpoints 

* [diffusers/ms-text-to-video-sd](https://huggingface.co/diffusers/ms-text-to-video-sd/)
* [diffusers/ms-text-to-video-1.7b](https://huggingface.co/diffusers/ms-text-to-video-1.7b)

## TextToVideoMSPipeline
[[autodoc]] TextToVideoMSPipeline
	- all
	- __call__