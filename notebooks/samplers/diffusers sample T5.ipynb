{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eb335cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "from diffusers import utils\n",
    "from diffusers import UNet2DConditionModel\n",
    "from diffusers import AudioLDMPipeline\n",
    "from diffusers import DDIMScheduler, PNDMScheduler\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "import csv\n",
    "import IPython.display as ipd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import argparse, os, sys, datetime, glob, importlib, csv\n",
    "import math\n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "# import cv2\n",
    "import torchaudio\n",
    "import torchaudio.transforms as transforms\n",
    "import time\n",
    "from encodec import EncodecModel\n",
    "from encodec.utils import convert_audio\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, T5EncoderModel,T5TokenizerFast\n",
    "from torchsummary import summary\n",
    "from IPython.display import FileLink\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e75388a-a8ce-466e-b92f-d37ae7aaaba3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "channel_means = [  2.2741,  11.2872,  -3.3938,  -1.5556,  -0.0302,   7.6089,  -5.5797,\n",
    "          0.2140,  -0.3536,   6.0188,   1.8582,  -0.1103,   2.2026,  -7.0081,\n",
    "         -0.0721,  -8.7742,  -2.4182,   4.4447,  -0.2184,  -0.5209, -11.9494,\n",
    "         -4.0776,  -1.4555,  -1.6505,   6.4522,   0.0997,  10.4067,  -3.9268,\n",
    "         -7.0161,  -3.1253,  -8.5145,   3.1156,   2.2279,  -5.2728,   2.8541,\n",
    "         -3.3980,  -1.1775,  -9.7662,   0.3048,   3.8765,   4.5021,   2.6239,\n",
    "         14.1057,   3.2852,   1.9702,  -1.6345,  -4.3733,   3.8198,   1.1421,\n",
    "         -4.4388,  -5.3498,  -6.6044,  -0.4426,   2.8000,  -7.0858,   2.4989,\n",
    "         -1.4915,  -6.1275,  -3.0896,   1.1227,  -8.7984,  -4.9831,  -0.3888,\n",
    "         -3.1017,  -7.5745,  -2.4760,   1.0540,  -2.5350,   0.0999,   0.6126,\n",
    "         -1.2301,  -5.8328,  -0.7275,  -1.2316,  -2.2532, -11.5017,   0.9166,\n",
    "         -2.2268,  -2.8496,  -0.5093,  -0.3037,  -6.3689,  -9.5225,   4.5965,\n",
    "          3.1329,  -1.8315,   5.3135,  -3.8361,   1.6335,  -0.1705,  11.0513,\n",
    "          5.3907,  -0.2660,   4.6109,  -8.9019,   6.5515,   0.8596,  16.6196,\n",
    "         -0.7732,   4.1237,   2.9267,   9.9652,   4.6615,   1.4660,  -9.7225,\n",
    "         -1.5841,  -0.5714,  -4.3343,  -0.1914,   2.8624, -11.2139,  -2.5840,\n",
    "         -6.7120,   0.2601,  -5.4195,   0.3554,   3.0438,  -1.0295,   1.3360,\n",
    "         -4.1767,   0.6468,   1.8145,   1.7140,   3.0185,   0.4881,   0.5796,\n",
    "         -2.4755,   2.6202]\n",
    "channel_stds = [1.7524, 1.2040, 1.1098, 1.1021, 1.3688, 1.1374, 1.8660, 0.9791, 1.4331,\n",
    "        1.7740, 1.2690, 1.0297, 0.9953, 1.5363, 1.2166, 1.6564, 1.4858, 1.2349,\n",
    "        1.5086, 1.0814, 1.4421, 0.9258, 0.9343, 1.2007, 1.3848, 1.2732, 1.7759,\n",
    "        1.3544, 1.4707, 1.2685, 1.7004, 1.2947, 1.2967, 1.8925, 0.9231, 0.7637,\n",
    "        1.3777, 1.6680, 0.9658, 0.9257, 0.5259, 0.9949, 1.7375, 1.0734, 1.2916,\n",
    "        0.8570, 0.6263, 0.9911, 0.9574, 0.9979, 1.5969, 1.1886, 1.1147, 1.2280,\n",
    "        2.0169, 1.1813, 1.2589, 1.1162, 1.3689, 1.2516, 1.2139, 1.0343, 1.1895,\n",
    "        1.1726, 1.1923, 1.2714, 1.0043, 0.6465, 1.3860, 1.4449, 0.9567, 1.0218,\n",
    "        0.9560, 1.4757, 1.0544, 0.8112, 1.4364, 1.0843, 1.2569, 1.0138, 1.1886,\n",
    "        0.8627, 1.1016, 1.4231, 1.3607, 1.1215, 1.9759, 1.5381, 0.9219, 0.8572,\n",
    "        0.6288, 0.8029, 1.1699, 1.1962, 1.5783, 0.9037, 1.2214, 2.0878, 1.3015,\n",
    "        1.2254, 1.2898, 1.5421, 1.2834, 1.7237, 1.3471, 0.8689, 1.2807, 1.2174,\n",
    "        1.2048, 0.6644, 1.5379, 1.4997, 0.7932, 0.7638, 0.8680, 1.3108, 1.8261,\n",
    "        1.3964, 1.2147, 1.1391, 1.0011, 1.5988, 1.5721, 1.0963, 1.4303, 1.3737,\n",
    "        1.5043, 1.3079]\n",
    "\n",
    "\n",
    "def show_split_img(image, title=None):\n",
    "    # Split the image in half horizontally\n",
    "    height, width = image.shape\n",
    "    half_width = width // 2\n",
    "    left_half = image[:, :half_width]\n",
    "    right_half = image[:, half_width:]\n",
    "\n",
    "    # Create the subplots and plot the left and right halves of the image\n",
    "    fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(16, 2))\n",
    "    axs[0].imshow(left_half)\n",
    "    axs[1].imshow(right_half)\n",
    "\n",
    "    # Hide the x and y axis ticks and labels for a cleaner plot\n",
    "    for ax in axs:\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "    # Show the plot\n",
    "    # if (title):\n",
    "    #     axs[0].title(title)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def rescale(image):\n",
    "    image = torch.Tensor.view(image, [128, 24, 21]).clone()\n",
    "    mean_v = torch.tensor(channel_means)\n",
    "    std_v =torch.tensor(channel_stds)\n",
    "    image = image * std_v.view(-1,1,1) + mean_v.view(-1,1,1)\n",
    "    \n",
    "    # for row, mean, std in zip(image, channel_means, channel_stds):\n",
    "    #     row *= std \n",
    "    #     row += mean\n",
    "    \n",
    "    image = torch.Tensor.view(image, [1, 128, 504])\n",
    "    return image\n",
    "\n",
    "def visualize_channels(img):\n",
    "    test_img = img.clone()\n",
    "    test_img = torch.Tensor.view(test_img, [128, 24, 21])\n",
    "    fig, axs = plt.subplots(8, 16, figsize=(12, 12))\n",
    "    for ax, layer in zip(axs.flatten(), test_img):\n",
    "        ax.imshow(layer.cpu().numpy())\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be76ab67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/u/li19/data_folder/model_cache/audio_journey_128\"\n",
    "device = \"cuda\"\n",
    "# model_path = \"CompVis/stable-diffusion-v1-4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01e6222e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/li19/data_folder/anaconda3/envs/diffuse/lib/python3.10/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=None, \n",
    "    safety_checker=None,\n",
    "    low_cpu_mem_usage=False)\n",
    "\n",
    "pipe.to(device)\n",
    "pipe.vae_scale_factor = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e423adba-0ab6-4ccc-b316-3ecadb5efcaf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.final_layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-large\", model_max_length=512)\n",
    "text_encoder = T5EncoderModel.from_pretrained(\"t5-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d5f80d4-8f89-47fa-a8a9-ad90368d7eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder.config.use_attention_mask = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b1b5276-87f9-4d82-b34a-70068de15a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(hasattr(text_encoder.config, \"use_attention_mask\") and text_encoder.config.use_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc999ee2-224b-484c-8b30-6a6e4b88b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe.tokenizer = tokenizer\n",
    "pipe.text_encoder = text_encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d9f1e69-6a4e-4025-950c-66f6d57d6598",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncodecModel.encodec_model_24khz()\n",
    "model.set_target_bandwidth(6.0)\n",
    "model = model.to(device)\n",
    "# pipe.vae = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "935139d4-ef34-4dfb-9bbb-52a74809ce3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# conf = \"/u/li19/data_folder/testing/audio_journey_128_channel_norm/checkpoint-34000/unet/\"\n",
    "conf = \"/u/li19/data_folder/testing/audio_journey_t5_34k_restart/checkpoint-43500/unet/\"\n",
    "\n",
    "new_unet = UNet2DConditionModel.from_pretrained(conf)\n",
    "new_unet.to(device)\n",
    "pipe.unet = new_unet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc3b291c-1105-4b7a-9430-869993446e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PNDMScheduler {\n",
      "  \"_class_name\": \"PNDMScheduler\",\n",
      "  \"_diffusers_version\": \"0.15.1\",\n",
      "  \"beta_end\": 0.012,\n",
      "  \"beta_schedule\": \"squaredcos_cap_v2\",\n",
      "  \"beta_start\": 0.00085,\n",
      "  \"clip_sample\": false,\n",
      "  \"num_train_timesteps\": 1000,\n",
      "  \"prediction_type\": \"epsilon\",\n",
      "  \"set_alpha_to_one\": false,\n",
      "  \"skip_prk_steps\": true,\n",
      "  \"steps_offset\": 1,\n",
      "  \"trained_betas\": null\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pipe.scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8dca9f2e-7308-4c84-9dce-e88c532abbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scheduler = DDIMScheduler.from_pretrained(\"/u/li19/data_folder/model_cache/audio_journey_128_ddim_2\", subfolder=\"scheduler\")\n",
    "pipe.scheduler = noise_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df9117fb-eed9-4d37-a950-b228f9a292ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise_scheduler = PNDMScheduler.from_pretrained(\"/u/li19/data_folder/model_cache/audio_journey_128\", subfolder=\"scheduler\")\n",
    "# pipe.scheduler = noise_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5590d77e-720a-42f1-bb78-eb383e769bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for downsample_block in pipe.unet.down_blocks:\n",
    "#         if hasattr(downsample_block, \"has_cross_attention\") and downsample_block.has_cross_attention:\n",
    "#             # sample, res_samples = downsample_block(\n",
    "#             #     hidden_states=sample,\n",
    "#             #     temb=emb,\n",
    "#             #     encoder_hidden_states=encoder_hidden_states,\n",
    "#             #     attention_mask=attention_mask,\n",
    "#             #     cross_attention_kwargs=cross_attention_kwargs,\n",
    "#             # )\n",
    "#             print(downsample_block.attention_mask)\n",
    "#         else:\n",
    "#             print('no cross attention')\n",
    "#             # sample, res_samples = downsample_block(hidden_states=sample, temb=emb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "48938168-dc97-4821-920d-27c2fec17b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5., 10., 15., 20., 25., 30., 35., 40., 45., 50., 55., 60., 65.])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "steps = torch.arange(65/5) * 5 + 5\n",
    "\n",
    "print(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "f68dc773-924b-4f59-85ca-0decb0e22022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(step, timestep, latents):\n",
    "    # img = rescale(latents[0].detach().cpu())\n",
    "    img = latents[0].detach().cpu()\n",
    "    plt.figure(figsize=(20,8))\n",
    "\n",
    "    # Remove tick marks\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(img[0].cpu().numpy())\n",
    "    filename = f'imgs_step_2/{step}.png'\n",
    "    plt.savefig(filename)\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print(step, timestep, latents.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "4e4eb210-d135-4a8b-8ec2-9868eeaac7d1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pipe(\n",
    "#     prompt=\"sound of a dog barking\", \n",
    "#     width=504, \n",
    "#     height=128, \n",
    "#     num_inference_steps=int(step), \n",
    "#     guidance_scale=6.5, \n",
    "#     output_type=\"latent\", \n",
    "#     unet_mask=False, \n",
    "#     debug=False,\n",
    "#     callback=callback,\n",
    "#     callback_steps=5\n",
    "# ).images[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dda86ae2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6df4287fe53d4bd5bdf5d0210d875563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3546f5602b84c28ad7374f33e6bcdca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcdde5615f194ce7a6907f3cdc351aaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ef082dec097410780e05a93aedb9693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71cdcf85186345a99e60af4e7e32d311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44effe6edec4413990bc04c970ef9cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61ce05811d7b4ebd97541a20fffd69ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "786ffa35d424466fbbed00f2b3b699f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompts = [\n",
    "# \"sound of a person speaking while a car passes\",\n",
    "          # \"a dog howling\",\n",
    "    \"sound of a cat meowing\",\n",
    "          \"sound of a dog barking\",\n",
    "           \"A woman giving a speech to a roomful of people\",\n",
    "           \"sound of A Hammer striking metal\",\n",
    "            \"a person is snoring while someone else is speaking.\",\n",
    "        \"The sound of Brass instrument together with piano, playing very sad music\",\n",
    "            \"The sound of Brass instrument together with piano, playing very happy music\",\n",
    "\n",
    "    \"A train whistle heard in the distance\",\n",
    "           # \"sound of Brass instrument, and Saxophone, and jazz\"\n",
    "          ]\n",
    "\n",
    "\n",
    "\n",
    "# imgs = [\n",
    "#     pipe(\n",
    "#         prompt=\"sound of a dog barking\", \n",
    "#         width=504, \n",
    "#         height=128, \n",
    "#         num_inference_steps=45, \n",
    "#         guidance_scale=4.5, \n",
    "#         output_type=\"latent\", \n",
    "#         unet_mask=False, \n",
    "#         debug=False).images[0] \n",
    "#     for step in steps]\n",
    "\n",
    "# imgs = [pipe(prompt=p, width=504, height=128, num_inference_steps=25, guidance_scale=4, output_type=\"latent\").images[0] for p in prompts]\n",
    "\n",
    "imgs = [pipe(prompt=p, width=504, height=128, num_inference_steps=50, guidance_scale=6.5, output_type=\"latent\", unet_mask=False, debug=False).images[0] for p in prompts]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(imgs[0].mean())\n",
    "# print(imgs[0].std())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612228c4-8809-4964-bd36-45faccf7aac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10\n",
    "shapes = [\n",
    "    (64, 512),\n",
    "    (8, 504),\n",
    "    (128, 504)\n",
    "]\n",
    "prompt = \"generate sound of a Livestock, farm animals, working animals\"\n",
    "pipe.unet.do_reshape = True\n",
    "final_times = []\n",
    "for shape in shapes:\n",
    "    print(shape)\n",
    "    dts = []\n",
    "    for _ in range(3):\n",
    "        t_0 = time.time()\n",
    "        for i in range(n_samples):\n",
    "            img = pipe(prompt=prompt, width=shape[1], height=shape[0], num_inference_steps=50, guidance_scale=6.5, output_type=\"latent\", unet_mask=False, debug=False)\n",
    "            output = model.decoder(img[0][0])\n",
    "        t_1 = time.time()\n",
    "        dt = t_1 - t_0\n",
    "        dts.append(dt)\n",
    "    final_times.append(np.array(dts).min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8eb91282-d673-4f06-8117-cf008ec67d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65536\n"
     ]
    }
   ],
   "source": [
    "a = 128 * 32 * 16\n",
    "b = 65536\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "42b04a49-45ed-451f-b684-44f192773a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 504])\n"
     ]
    }
   ],
   "source": [
    "print(imgs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "ab2aa30b-a326-4fcd-803a-5fb2e93f5f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 24\n"
     ]
    }
   ],
   "source": [
    "def closest_factors(x):\n",
    "    root = math.sqrt(x)\n",
    "    lower = math.floor(root)\n",
    "    upper = math.ceil(root)\n",
    "    for i in range(lower, 0, -1):\n",
    "        if x % i == 0:\n",
    "            return (i, x//i)\n",
    "    for i in range(upper, x+1):\n",
    "        if x % i == 0:\n",
    "            return (i, x//i)\n",
    "targ_height, targ_width = closest_factors(imgs[0].shape[-1])\n",
    "print(targ_height, targ_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccb84c37-aaa7-4de7-a5d7-e1a1368202b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_imgs = [rescale(img[0].detach().cpu()) for img in imgs]\n",
    "# norm_imgs = [img[0] for img in imgs]\n",
    "# d_imgs = [imgs[i].detach().cpu() - norm_imgs[i] for i in range(len(prompts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61d381c0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'steps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# print(\"================================MULTI PROMPT================================\")\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(norm_imgs, \u001b[43msteps\u001b[49m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(step)\n\u001b[1;32m      4\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m8\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'steps' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(\"================================MULTI PROMPT================================\")\n",
    "for img, step in zip(norm_imgs, steps):\n",
    "    print(step)\n",
    "    plt.figure(figsize=(20,8))\n",
    "    plt.imshow(img[0].cpu().numpy())\n",
    "    filename = f'imgs_step/{step}.png'\n",
    "\n",
    "    plt.savefig(filename)\n",
    "\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    display(FileLink(filename))\n",
    "\n",
    "    \n",
    "# for img, cap in zip(imgs, prompts):\n",
    "#     print(cap)\n",
    "#     plt.figure(figsize=(20,8))\n",
    "#     plt.imshow(img[0].cpu().numpy())\n",
    "\n",
    "#     plt.colorbar()\n",
    "#     plt.show()\n",
    "    # show_split_img(img[0,:,:].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daafffa7-d189-43e2-8189-1b52d2891b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for vanilla in imgs:\n",
    "    img = torch.Tensor.view(vanilla.clone(), [128, 24, 21])\n",
    "    means = img.mean(axis=(1,2))\n",
    "    stds = img.std(axis=(1,2))\n",
    "    \n",
    "    print(f'MEAN: mean {means.mean()} -- std {means.std()}')\n",
    "    print(f'STD: mean {stds.mean()} -- std {stds.std()}')\n",
    "    print()\n",
    "#     print(means.mean())\n",
    "#     print(stds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d72c6b-3e5a-4818-9c18-02b86e08de38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for scaled, vanilla in zip(norm_imgs, imgs):\n",
    "    print(\"======= STATS =======\")\n",
    "    print(f'Scaled - Mean {scaled.mean()} : STD: {scaled.std()}')\n",
    "    print(f'Vanilla - Mean {vanilla.mean()} : STD: {vanilla.std()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f91641e0-2fea-4ce9-909b-0973a4c59430",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m idx \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      6\u001b[0m sample \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcopy(norm_imgs[idx]\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mimg\u001b[49m\u001b[38;5;241m.\u001b[39mmean())\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(img\u001b[38;5;241m.\u001b[39mstd())\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# img /= std\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# img += mean\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'img' is not defined"
     ]
    }
   ],
   "source": [
    "# mean = -0.50601\n",
    "# std = 5.22701\n",
    "\n",
    "idx =0\n",
    "\n",
    "sample = np.copy(norm_imgs[idx].cpu())\n",
    "\n",
    "\n",
    "print(img.mean())\n",
    "print(img.std())\n",
    "\n",
    "\n",
    "# img /= std\n",
    "# img += mean\n",
    "\n",
    "print(\"==== POST SCALE ====\")\n",
    "print(img.mean())\n",
    "print(img.std())\n",
    "\n",
    "\n",
    "quantized = model.quantizer(torch.from_numpy(sample).to(device), model.frame_rate).quantized\n",
    "\n",
    "print(img.shape)\n",
    "t_img = torch.from_numpy(sample).to(device)\n",
    "output = model.decoder(t_img)\n",
    "output_2 = model.decoder(quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb5cafb0-f9c5-42b9-88e0-3460b8f7d28b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43moutput\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdd9a05d-ee1d-4cd0-9cef-f4336b2a49e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m final \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(prompts[idx])\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(final)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "final = output[0][0].detach().cpu().numpy()\n",
    "\n",
    "print(prompts[idx])\n",
    "\n",
    "plt.plot(final)\n",
    "ipd.Audio(final, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce77ed3-9d9e-4dd0-b0f4-ec9a57b18665",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# t_img = torch.from_numpy(img).to(\"cuda\")\n",
    "# quantized = model.quantizer.encode(t_img, model.frame_rate, model.bandwidth)\n",
    "# print(quantized.shape)\n",
    "# # plt.imshow(quantized.cpu().numpy())\n",
    "# decode = model.quantizer.decode(quantized)\n",
    "# print(decode.shape)\n",
    "# plt.imshow(decode[0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db60af3b-1314-4914-a4da-6e6b69a7aaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fft = 4096\n",
    "win_length = 1024\n",
    "hop_length = 400\n",
    "n_mels = 64\n",
    "sample_rate = 16000\n",
    "\n",
    "waveform = torch.from_numpy(np.copy(final)[None, :])\n",
    "\n",
    "print(waveform.shape)\n",
    "\n",
    "mel_spectrogram_transform = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=sample_rate,\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "    center=True,\n",
    "    pad_mode=\"constant\",\n",
    "    power=2.0,\n",
    "    norm=\"slaney\",\n",
    "    onesided=True,\n",
    "    n_mels=n_mels,\n",
    "    mel_scale=\"slaney\",\n",
    ")\n",
    "\n",
    "mel_spectrogram = mel_spectrogram_transform(waveform)\n",
    "amplitude_to_db_transform = torchaudio.transforms.AmplitudeToDB(stype='power', top_db=88)\n",
    "log_mel_spectrogram_db = amplitude_to_db_transform(mel_spectrogram)\n",
    "# log_mel_spectrogram_db = log_mel_spectrogram_db.transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3f072e-c97e-4572-9508-b31934a60a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size\n",
    "fig= plt.figure(figsize=(20, 10))\n",
    "# Show the image\n",
    "plt.imshow(log_mel_spectrogram_db[0], origin='lower')\n",
    "filename = f'imgs/spectrogram_{prompts[idx]}.png'\n",
    "plt.savefig(filename)\n",
    "plt.show()\n",
    "display(FileLink(filename))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46f7e8b-ad85-48ba-a3e1-d2a8a8629ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffuse",
   "language": "python",
   "name": "diffuse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
